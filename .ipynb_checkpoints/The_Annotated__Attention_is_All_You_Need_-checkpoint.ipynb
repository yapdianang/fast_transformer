{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wArePITKUgQG"
   },
   "source": [
    "<img src=\"aiayn.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tSWEk4ttUgQH"
   },
   "source": [
    "> When teaching, I emphasize implementation as a way to understand recent developments in ML. This post is an attempt to keep myself honest along this goal. The recent [\"Attention is All You Need\"]\n",
    "(https://arxiv.org/abs/1706.03762) paper from NIPS 2017 has been instantly impactful paper as a new method for machine translation and potentiall NLP generally. The paper is very clearly written, but the conventional wisdom has been that it is quite difficult to implement correctly. \n",
    ">\n",
    "> In this post I follow the paper through from start to finish and try to implement each component in code. \n",
    "(I have done some minor reordering and skipping from the original paper). This document itself is a working notebook, and should be a completely usable and efficient implementation. To follow along you will first need to install [PyTorch](http://pytorch.org/) and [torchtext](https://github.com/pytorch/text). The complete code is available on [github](https://github.com/harvardnlp/annotated-transformer).\n",
    ">- Alexander \"Sasha\" Rush ([@harvardnlp](https://twitter.com/harvardnlp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4LTc4HW7UgQI"
   },
   "outputs": [],
   "source": [
    "# Standard PyTorch imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# For plots\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "esxhOQubUgQL"
   },
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1M-PiEMOUgQM"
   },
   "source": [
    "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
    "[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\n",
    "block, computing hidden representations in parallel for all input and output positions. In these models,\n",
    "the number of operations required to relate signals from two arbitrary input or output positions grows\n",
    "in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\n",
    "it more difficult to learn dependencies between distant positions [12]. In the Transformer this is\n",
    "reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
    "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
    "described in section 3.2.\n",
    "\n",
    "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
    "of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
    "used successfully in a variety of tasks including reading comprehension, abstractive summarization,\n",
    "textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
    "End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned\n",
    "recurrence and have been shown to perform well on simple-language question answering and\n",
    "language modeling tasks [34].\n",
    "\n",
    "To the best of our knowledge, however, the Transformer is the first transduction model relying\n",
    "entirely on self-attention to compute representations of its input and output without using sequencealigned\n",
    "RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
    "self-attention and discuss its advantages over models such as [17, 18] and [9]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84vTAA5TUgQM"
   },
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f-f9BuNsUgQN"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VBOvyU9BUgQN"
   },
   "source": [
    "Most competitive neural sequence transduction models have an encoder-decoder structure [(cite)](cho2014learning,bahdanau2014neural,sutskever14). Here, the encoder maps an input sequence of symbol representations $(x_1, ..., x_n)$ to a sequence of continuous representations $\\mathbf{z} = (z_1, ..., z_n)$. Given $\\mathbf{z}$, the decoder then generates an output sequence $(y_1,...,y_m)$ of symbols one element at a time. At each step the model is auto-regressive [(cite)](graves2013generating), consuming the previously generated symbols as additional input when generating the next. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1AC8KeDJUgQO"
   },
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base model for this and many \n",
    "    other models.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        memory = self.encoder(self.src_embed(src), src_mask)\n",
    "        output = self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ip0EXqvEUgQQ"
   },
   "source": [
    "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q9dznVhQUgQQ"
   },
   "source": [
    "<img src=\"ModalNet-21.png\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "euyRXbaMUgQR"
   },
   "source": [
    "## Encoder and Decoder Stacks   \n",
    "\n",
    "### Encoder: \n",
    "\n",
    "The encoder is composed of a stack of $N=6$ identical layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6yy7pY85UgQR"
   },
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "psiq5idJUgQT"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mie9sUeSUgQV"
   },
   "source": [
    "We employ a residual connection [(cite)](he2016deep) around each of the two sub-layers, followed by layer normalization [(cite)](layernorm2016).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sEz9kLClUgQV"
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nx4On5PCUgQY"
   },
   "source": [
    "That is, the output of each sub-layer is $\\mathrm{LayerNorm}(x + \\mathrm{Sublayer}(x))$, where $\\mathrm{Sublayer}(x)$ is the function implemented by the sub-layer itself.  We apply dropout [(cite)](srivastava2014dropout) to the output of each sub-layer, before it is added to the sub-layer input and normalized.  \n",
    "\n",
    "To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension $d_{\\text{model}}=512$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zx9JBwAcUgQY"
   },
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity we apply the norm first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer function that maintains the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZduB6mIlUgQa"
   },
   "source": [
    "Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0mEBw9tIUgQb"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of two sublayers, self-attn and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iHOZnpnBUgQd"
   },
   "source": [
    "### Decoder:\n",
    "\n",
    "The decoder is also composed of a stack of $N=6$ identical layers.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3o_ZB42sUgQd"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dOi_W1qaUgQf"
   },
   "source": [
    "In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.  Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kMm6xHWVUgQg"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"Decoder is made up of three sublayers, self-attn, src-attn, and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    " \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"Follow Figure 1 (right) for connections.\"\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Nen9h7wUgQi"
   },
   "source": [
    "We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions.  This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position $i$ can depend only on the known outputs at positions less than $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RyQKI9AgUgQj"
   },
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "colab_type": "code",
    "id": "fgQMtvM-UgQl",
    "outputId": "07611646-88f8-4410-8bd9-77bbe59b86cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x113149be0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11305e6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The attention mask shows the position each tgt word (row) is allowed to look at (column).\n",
    "# Words are blocked for attending to future words during training. \n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(subsequent_mask(20)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fl2E1IaqUgQq"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3twSbimFUgQq"
   },
   "source": [
    "### Attention:                                                                                                                                                                                                                                                                               \n",
    "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors.  The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.                                                                                                                                                                                                                                                                                           \n",
    "\n",
    "We call our particular attention \"Scaled Dot-Product Attention\".   The input consists of queries and keys of dimension $d_k$, and values of dimension $d_v$.  We compute the dot products of the query with all keys, divide each by $\\sqrt{d_k}$, and apply a softmax function to obtain the weights on the values.                                                                                                         \n",
    "<img width=\"220px\" src=\"ModalNet-19.png\">\n",
    "\n",
    "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix $Q$.   The keys and values are also packed together into matrices $K$ and $V$.  We compute the matrix of outputs as:                      \n",
    "                                                                 \n",
    "$$                                                                         \n",
    "   \\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V               \n",
    "$$                                                                                                                                                                                                        \n",
    "                                                                                                                                                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wlZ8zw9PUgQr"
   },
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=0.0):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    # (Dropout described below)\n",
    "    p_attn = F.dropout(p_attn, p=dropout)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AV7cIqbrUgQs"
   },
   "source": [
    "The two most commonly used attention functions are additive attention [(cite)](bahdanau2014neural), and dot-product (multiplicative) attention.  Dot-product attention is identical to our algorithm, except for the scaling factor of $\\frac{1}{\\sqrt{d_k}}$. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer.  While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.                                                                                             \n",
    "\n",
    "                                                                        \n",
    "While for small values of $d_k$ the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of $d_k$ [(cite)](DBLP:journals/corr/BritzGLL17). We suspect that for large values of $d_k$, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients  (To illustrate why the dot products get large, assume that the components of $q$ and $k$ are independent random variables with mean $0$ and variance $1$.  Then their dot product, $q \\cdot k = \\sum_{i=1}^{d_k} q_ik_i$, has mean $0$ and variance $d_k$.). To counteract this effect, we scale the dot products by $\\frac{1}{\\sqrt{d_k}}$.          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OV7kNMbKUgQt"
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uiaCxaGGUgQt"
   },
   "source": [
    "### Multi-Head Attention                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
    "Instead of performing a single attention function with $d_{\\text{model}}$-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values $h$ times with different, learned linear projections to $d_k$, $d_k$ and $d_v$ dimensions, respectively.                                                                                                                                                                                                   \n",
    "On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding $d_v$-dimensional output values. These are concatenated and once again projected, resulting in the final values:\n",
    "\n",
    "<img width=\"270px\" src=\"ModalNet-20.png\">\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
    "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.                                                                                                                                                                                                                                                                                             \n",
    "    \n",
    "    \n",
    "   \n",
    "$$    \n",
    "\\mathrm{MultiHead}(Q, K, V) = \\mathrm{Concat}(\\mathrm{head_1}, ..., \\mathrm{head_h})W^O    \\\\                                           \n",
    "    \\text{where}~\\mathrm{head_i} = \\mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)                                \n",
    "$$                                                                                                                                                                                                                                                                                                                                                                         \n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
    "Where the projections are parameter matrices $W^Q_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$, $W^K_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$, $W^V_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_v}$ and $W^O \\in \\mathbb{R}^{hd_v \\times d_{\\text{model}}}$.                                                                                                                                                                                                                                                        \n",
    "   \n",
    "\n",
    "   \n",
    "In this work we employ $h=8$ parallel attention layers, or heads. For each of these we use $d_k=d_v=d_{\\text{model}}/h=64$. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_ea0UrEgUgQt"
   },
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.p = dropout\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "                             for l, x in zip(self.linears, (query, key, value))]\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.p)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aVdpQ_KwUgQv"
   },
   "source": [
    "### Applications of Attention in our Model                                                                                                                                                      \n",
    "The Transformer uses multi-head attention in three different ways:                                                        \n",
    "1) In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.   This allows every position in the decoder to attend over all positions in the input sequence.  This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [(cite)](wu2016google, bahdanau2014neural,JonasFaceNet2017).    \n",
    "\n",
    "\n",
    "2) The encoder contains self-attention layers.  In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder.   Each position in the encoder can attend to all positions in the previous layer of the encoder.                                                   \n",
    "\n",
    "\n",
    "3) Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.  We need to prevent leftward information flow in the decoder to preserve the auto-regressive property.  We implement this inside of scaled dot-product attention by masking out (setting to $-\\infty$) all values in the input of the softmax which correspond to illegal connections.                                                                                                                                                                                                                                                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gERLhK-FUgQw"
   },
   "source": [
    "## Position-wise Feed-Forward Networks                                                                                                                                                                                                                                                                                                                                                             \n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
    "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.  This consists of two linear transformations with a ReLU activation in between.\n",
    "\n",
    "$$\\mathrm{FFN}(x)=\\max(0, xW_1 + b_1) W_2 + b_2$$                                                                                                                                                                                                                                                         \n",
    "                                                                                                                                                                                                                                                        \n",
    "While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1.  The dimensionality of input and output is $d_{\\text{model}}=512$, and the inner-layer has dimensionality $d_{ff}=2048$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HuDPthO2UgQx"
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        # Torch linears have a `b` by default. \n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "68VLwifsUgQz"
   },
   "source": [
    "## Embeddings and Softmax                                                                                                                                                                                                                                                                                           \n",
    "Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension $d_{\\text{model}}$.  We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities.  In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [(cite)](press2016using). In the embedding layers, we multiply those weights by $\\sqrt{d_{\\text{model}}}$.                                                                                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sl5JzPeGUgQz"
   },
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F_hw5TyCUgQ1"
   },
   "source": [
    "## Positional Encoding                                                                                                                             \n",
    "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence.  To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks.  The positional encodings have the same dimension $d_{\\text{model}}$ as the embeddings, so that the two can be summed.   There are many choices of positional encodings, learned and fixed [(cite)](JonasFaceNet2017). \n",
    "\n",
    "In this work, we use sine and cosine functions of different frequencies:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
    "$$                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
    "    PE_{(pos,2i)} = sin(pos / 10000^{2i/d_{\\text{model}}}) \\\\                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
    "    PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_{\\text{model}}})                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
    "$$                                                                                                                                                                                                                                                        \n",
    "where $pos$ is the position and $i$ is the dimension.  That is, each dimension of the positional encoding corresponds to a sinusoid.  The wavelengths form a geometric progression from $2\\pi$ to $10000 \\cdot 2\\pi$.  We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset $k$, $PE_{pos+k}$ can be represented as a linear function of $PE_{pos}$. \n",
    "\n",
    "In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks.  For the base model, we use a rate of $P_{drop}=0.1$. \n",
    "                                                                                                                                                                                                                                                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MVsjhp6uUgQ1"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0., max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0., d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "colab_type": "code",
    "id": "qMsBRCuLUgQ3",
    "outputId": "0ba52f87-93d6-441b-b6c6-b724981758db"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1132bd5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The positional encoding will add in a sine wave based on position.\n",
    "# The frequency and offset of the wave is different for each dimension.\n",
    "plt.figure(figsize=(15, 5))\n",
    "pe = PositionalEncoding(20, 0)\n",
    "y = pe.forward(Variable(torch.zeros(1, 100, 20)))\n",
    "plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())\n",
    "plt.legend([\"dim %d\"%p for p in [4,5,6,7]])\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6HjtmgH1UgQ5"
   },
   "source": [
    "We also experimented with using learned positional embeddings [(cite)](JonasFaceNet2017) instead, and found that the two versions produced nearly identical results.  We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZtnFnHH9UgQ6"
   },
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "heaKRIaZUgQ6"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"Standard generation step. (Not described in the paper.)\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0i97-Y7AUgQ8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dd3lP9fTUgQ9"
   },
   "source": [
    "## Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b-LDhRoaUgQ-"
   },
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    \"Construct a model object based on hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model, dropout)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab))\n",
    "    \n",
    "    # This was important from their code. Initialize parameters with Glorot or fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3023
    },
    "colab_type": "code",
    "id": "qP-g4KfhUgQ_",
    "outputId": "e5d671a2-a9d4-461b-f08c-bf5b7a4ddb73"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm()\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm()\n",
       "  )\n",
       "  (src_embed): Sequential(\n",
       "    (0): Embeddings(\n",
       "      (lut): Embedding(10, 512)\n",
       "    )\n",
       "    (1): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (tgt_embed): Sequential(\n",
       "    (0): Embeddings(\n",
       "      (lut): Embedding(10, 512)\n",
       "    )\n",
       "    (1): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (generator): Generator(\n",
       "    (proj): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Small example model.\n",
    "tmp_model = make_model(10, 10, 2)\n",
    "tmp_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MuV1e8nEUgRB"
   },
   "source": [
    "# Training\n",
    "\n",
    "This section describes the training regime for our models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OhVxtlZaUgRC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "naFpt_GUUgRD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X60DPvyaUgRF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lz6n3REAUgRH"
   },
   "source": [
    "## Training Data and Batching\n",
    "\n",
    "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.  Sentences were encoded using byte-pair encoding \\citep{DBLP:journals/corr/BritzGLL17}, which has a shared source-target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [(cite)](wu2016google). \n",
    "\n",
    "\n",
    "Sentence pairs were batched together by approximate sequence length.  Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PRoJURieUgRH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "52xwbbb4UgRK"
   },
   "source": [
    "## Hardware and Schedule                                                                                                                                                                                                   \n",
    "We trained our models on one machine with 8 NVIDIA P100 GPUs.  For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds.  We trained the base models for a total of 100,000 steps or 12 hours.  For our big models, step time was 1.0 seconds.  The big models were trained for 300,000 steps (3.5 days)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MPp__T_uUgRK"
   },
   "source": [
    "## Optimizer\n",
    "\n",
    "We used the Adam optimizer [(cite)](kingma2014adam) with $\\beta_1=0.9$, $\\beta_2=0.98$ and $\\epsilon=10^{-9}$.  We varied the learning rate over the course of training, according to the formula:                                                                                            \n",
    "$$                                                                                                                                                                                                                                                                                         \n",
    "lrate = d_{\\text{model}}^{-0.5} \\cdot                                                                                                                                                                                                                                                                                                \n",
    "  \\min({step\\_num}^{-0.5},                                                                                                                                                                                                                                                                                                  \n",
    "    {step\\_num} \\cdot {warmup\\_steps}^{-1.5})                                                                                                                                                                                                                                                                               \n",
    "$$                                                                                                                                                                                             \n",
    "This corresponds to increasing the learning rate linearly for the first $warmup\\_steps$ training steps, and decreasing it thereafter proportionally to the inverse square root of the step number.  We used $warmup\\_steps=4000$.                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q5yV0f2QUgRL"
   },
   "outputs": [],
   "source": [
    "# Note: This part is incredibly important. \n",
    "# Need to train with this setup of the model is very unstable.\n",
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "            min(step ** (-0.5), step * self.warmup**(-1.5)))\n",
    "        \n",
    "def get_std_opt(model):\n",
    "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
    "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FM5n1PJxUgRN"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "colab_type": "code",
    "id": "PC2wCVEFUgRO",
    "outputId": "b3fc5e15-3be4-4e1b-fb3b-fd722207a9a9"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11310b240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Three settings of the lrate hyperparameters.\n",
    "opts = [NoamOpt(512, 1, 4000, None), \n",
    "        NoamOpt(512, 1, 8000, None),\n",
    "        NoamOpt(256, 1, 4000, None)]\n",
    "plt.plot(np.arange(1, 20000), [[opt.rate(i) for opt in opts] for i in range(1, 20000)])\n",
    "plt.legend([\"512:4000\", \"512:8000\", \"256:4000\"])\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tkzxQYKUgRQ"
   },
   "source": [
    "## Regularization                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
    "                                                                                                                                                                                                                                                                                                                      \n",
    "### Label Smoothing\n",
    "\n",
    "During training, we employed label smoothing of value $\\epsilon_{ls}=0.1$ [(cite)](DBLP:journals/corr/SzegedyVISW15).  This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IVWDJLXrUgRQ"
   },
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    \"\"\"\n",
    "    Implement label smoothing.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False)\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0 and mask.size(-1) > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "colab_type": "code",
    "id": "_gXwt5HVUgRT",
    "outputId": "3ea1a660-0bc7-4b1e-ba8e-7d3fbf560640"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dianang/.local/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117316908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Example\n",
    "crit = LabelSmoothing(5, 0, 0.5)\n",
    "predict = torch.FloatTensor([[0, 0.2, 0.7, 0.1, 0],\n",
    "                             [0, 0.2, 0.7, 0.1, 0], \n",
    "                             [0, 0.2, 0.7, 0.1, 0]])\n",
    "v = crit(Variable(predict.log()), \n",
    "         Variable(torch.LongTensor([2, 1, 0])))\n",
    "\n",
    "# Show the target distributions expected by the system.\n",
    "plt.imshow(crit.true_dist)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 288
    },
    "colab_type": "code",
    "id": "sHBHTwmjUgRU",
    "outputId": "4086db46-2dd9-4566-804e-8e5f80641a69"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dianang/.local/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/Users/dianang/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1134cc2e8>]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117316940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Label smoothing starts to penalize the model \n",
    "# if it gets very confident about a given choice\n",
    "crit = LabelSmoothing(5, 0, 0.2)\n",
    "def loss(x):\n",
    "    d = x + 3 * 1\n",
    "    predict = torch.FloatTensor([[0, x / d, 1 / d, 1 / d, 1 / d],\n",
    "                                 ])\n",
    "    #print(predict)\n",
    "    return crit(Variable(predict.log()),\n",
    "                 Variable(torch.LongTensor([1]))).data[0]\n",
    "plt.plot(np.arange(1, 100), [loss(x) for x in range(1, 100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XoyfFgLoUgRW"
   },
   "source": [
    "### Memory Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yVKyONFsUgRW"
   },
   "outputs": [],
   "source": [
    "def loss_backprop(generator, criterion, out, targets, normalize):\n",
    "    \"\"\"\n",
    "    Memory optmization. Compute each timestep separately and sum grads.\n",
    "    \"\"\"\n",
    "    assert out.size(1) == targets.size(1)\n",
    "    total = 0.0\n",
    "    out_grad = []\n",
    "    for i in range(out.size(1)):\n",
    "        out_column = Variable(out[:, i].data, requires_grad=True)\n",
    "        gen = generator(out_column)\n",
    "        loss = criterion(gen, targets[:, i].float()) / normalize\n",
    "        total += loss.data[0]\n",
    "        loss.backward()\n",
    "        out_grad.append(out_column.grad.data.clone())\n",
    "    out_grad = torch.stack(out_grad, dim=1)\n",
    "    out.backward(gradient=out_grad)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LiTlbMq2UgRY"
   },
   "outputs": [],
   "source": [
    "def make_std_mask(src, tgt, pad):\n",
    "    src_mask = (src != pad).unsqueeze(-2)\n",
    "    tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "    tgt_mask = tgt_mask & Variable(subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "    return src_mask, tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sSF9AaKJUgRZ"
   },
   "outputs": [],
   "source": [
    "def train_epoch(train_iter, model, criterion, opt, transpose=False):\n",
    "    model.train()\n",
    "    for i, batch in enumerate(train_iter):\n",
    "        src, trg, src_mask, trg_mask = \\\n",
    "            batch.src, batch.trg, batch.src_mask, batch.trg_mask\n",
    "        out = model.forward(src, trg[:, :-1], src_mask, trg_mask[:, :-1, :-1])\n",
    "        loss = loss_backprop(model.generator, criterion, out, trg[:, 1:], batch.ntokens) \n",
    "                        \n",
    "        model_opt.step()\n",
    "        model_opt.optimizer.zero_grad()\n",
    "        if i % 10 == 1:\n",
    "            print(i, loss, model_opt._rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q4kFYs7nUgRb"
   },
   "outputs": [],
   "source": [
    "def valid_epoch(valid_iter, model, criterion, transpose=False):\n",
    "    model.test()\n",
    "    total = 0\n",
    "    for batch in valid_iter:\n",
    "        src, trg, src_mask, trg_mask = \\\n",
    "            batch.src, batch.trg, batch.src_mask, batch.trg_mask\n",
    "        out = model.forward(src, trg[:, :-1], src_mask, trg_mask[:, :-1, :-1])\n",
    "        loss = loss_backprop(model.generator, criterion, out, trg[:, 1:], batch.ntokens) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uJo5AZasUgRd"
   },
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    def __init__(self, src, trg, src_mask, trg_mask, ntokens):\n",
    "        self.src = src\n",
    "        self.trg = trg\n",
    "        self.src_mask = src_mask\n",
    "        self.trg_mask = trg_mask\n",
    "        self.ntokens = ntokens\n",
    "    \n",
    "def data_gen(V, batch, nbatches):\n",
    "    for i in range(nbatches):\n",
    "        data = torch.from_numpy(np.random.randint(1, V, size=(batch, 10)))\n",
    "        src = Variable(data, requires_grad=False)\n",
    "        tgt = Variable(data, requires_grad=False)\n",
    "        src_mask, tgt_mask = make_std_mask(src, tgt, 0)\n",
    "        yield Batch(src, tgt, src_mask, tgt_mask, (tgt[1:] != 0).data.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 604
    },
    "colab_type": "code",
    "id": "vrpU5b2sUgRe",
    "outputId": "00871f60-2fcb-4235-c0ab-8de02a0c069c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dianang/.local/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of type torch.LongTensor but found type torch.FloatTensor for argument #3 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-0edc84824e16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_std_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-53-1735085d52f5>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(train_iter, model, criterion, opt, transpose)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_mask\u001b[0m \u001b[0;34m=\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_backprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mntokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mmodel_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-2aabcd419698>\u001b[0m in \u001b[0;36mloss_backprop\u001b[0;34m(generator, criterion, out, targets, normalize)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mout_column\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-de6487f43cc7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, target)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mtrue_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtrue_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmoothing\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mtrue_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfidence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mtrue_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of type torch.LongTensor but found type torch.FloatTensor for argument #3 'index'"
     ]
    }
   ],
   "source": [
    "V = 11\n",
    "criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n",
    "model = make_model(V, V, N=2)\n",
    "model_opt = get_std_opt(model)\n",
    "for epoch in range(2):\n",
    "    train_epoch(data_gen(V, 30, 20), model, criterion, model_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5S2BcIoOUgRg"
   },
   "source": [
    "# A Real World Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aP_oq0kLUgRh"
   },
   "outputs": [],
   "source": [
    "# For data loading.\n",
    "from torchtext import data, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 989
    },
    "colab_type": "code",
    "id": "eKynVliVXg6F",
    "outputId": "f471f71a-d329-4fa9-d188-b4dcc9283234"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages\n",
      "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext)\n",
      "Requirement already satisfied: preshed<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
      "Requirement already satisfied: murmurhash<0.29,>=0.28 in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
      "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
      "Requirement already satisfied: cymem<1.32,>=1.30 in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
      "Requirement already satisfied: msgpack-python==0.5.4 in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
      "Requirement already satisfied: html5lib==1.0b8 in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
      "Requirement already satisfied: thinc<6.11.0,>=6.10.1 in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
      "Requirement already satisfied: pathlib in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
      "Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
      "Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
      "Requirement already satisfied: msgpack-numpy==0.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
      "Requirement already satisfied: ftfy<5.0.0,>=4.4.2 in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
      "Requirement already satisfied: regex==2017.4.5 in /usr/local/lib/python3.6/dist-packages (from spacy)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext)\n",
      "Requirement already satisfied: cytoolz<0.9,>=0.8 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy<5.0.0,>=4.4.2->spacy)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.9,>=0.8->thinc<6.11.0,>=6.10.1->spacy)\n",
      "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n",
      "\u001b[K    6% |██                              | 2.4MB 47.2MB/s eta 0:00:01\u001b[K    100% |████████████████████████████████| 37.4MB 50.2MB/s \n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "  Running setup.py install for en-core-web-sm ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed en-core-web-sm-2.0.0\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
      "    /usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
      "\n",
      "    You can now load the model via spacy.load('en')\n",
      "\n",
      "Collecting https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.0.0/de_core_news_sm-2.0.0.tar.gz\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.0.0/de_core_news_sm-2.0.0.tar.gz (38.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 38.2MB 50.0MB/s \n",
      "\u001b[?25hInstalling collected packages: de-core-news-sm\n",
      "  Running setup.py install for de-core-news-sm ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed de-core-news-sm-2.0.0\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /usr/local/lib/python3.6/dist-packages/de_core_news_sm -->\n",
      "    /usr/local/lib/python3.6/dist-packages/spacy/data/de\n",
      "\n",
      "    You can now load the model via spacy.load('de')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext spacy\n",
    "!python -m spacy download en\n",
    "!python -m spacy download de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "colab_type": "code",
    "id": "lXtYwdHqUgRj",
    "outputId": "31ee7819-a5eb-4cf2-cb06-cc7932bf535a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading de-en.tgz\n",
      ".data/iwslt/de-en/IWSLT16.TEDX.dev2012.de-en.de.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.tst2013.de-en.en.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.tst2014.de-en.de.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.tst2012.de-en.en.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.tst2010.de-en.en.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.dev2010.de-en.en.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.dev2010.de-en.de.xml\n",
      ".data/iwslt/de-en/IWSLT16.TEDX.tst2013.de-en.en.xml\n",
      ".data/iwslt/de-en/IWSLT16.TEDX.dev2012.de-en.en.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.tst2013.de-en.de.xml\n",
      ".data/iwslt/de-en/IWSLT16.TEDX.tst2013.de-en.de.xml\n",
      ".data/iwslt/de-en/IWSLT16.TEDX.tst2014.de-en.de.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.tst2014.de-en.en.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.tst2011.de-en.de.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.tst2011.de-en.en.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.tst2010.de-en.de.xml\n",
      ".data/iwslt/de-en/IWSLT16.TEDX.tst2014.de-en.en.xml\n",
      ".data/iwslt/de-en/IWSLT16.TED.tst2012.de-en.de.xml\n",
      ".data/iwslt/de-en/train.tags.de-en.en\n",
      ".data/iwslt/de-en/train.tags.de-en.de\n"
     ]
    }
   ],
   "source": [
    "# Load words from IWSLT\n",
    "\n",
    "#!pip install torchtext spacy\n",
    "#!python -m spacy download en\n",
    "#!python -m spacy download de\n",
    "\n",
    "import spacy\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "BLANK_WORD = \"<blank>\"\n",
    "SRC = data.Field(tokenize=tokenize_de, pad_token=BLANK_WORD)\n",
    "TGT = data.Field(tokenize=tokenize_en, init_token = BOS_WORD, \n",
    "                 eos_token = EOS_WORD, pad_token=BLANK_WORD)\n",
    "\n",
    "MAX_LEN = 100\n",
    "train, val, test = datasets.IWSLT.splits(exts=('.de', '.en'), fields=(SRC, TGT), \n",
    "                                         filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "                                         len(vars(x)['trg']) <= MAX_LEN)\n",
    "MIN_FREQ = 1\n",
    "SRC.build_vocab(train.src, min_freq=MIN_FREQ)\n",
    "TGT.build_vocab(train.trg, min_freq=MIN_FREQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "colab_type": "code",
    "id": "F8MTIJTWUgRl",
    "outputId": "bd6c163a-14a2-4f8c-b0d5-01a033f278c4"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a98b1f496512>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_elements\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_elements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMyIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# Detail. Batching seems to matter quite a bit. \n",
    "# This is temporary code for dynamic batching based on number of tokens.\n",
    "# This code should all go away once things get merged in this library.\n",
    "\n",
    "BATCH_SIZE = 4096\n",
    "global max_src_in_batch, max_tgt_in_batch\n",
    "def batch_size_fn(new, count, sofar):\n",
    "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
    "    global max_src_in_batch, max_tgt_in_batch\n",
    "    if count == 1:\n",
    "        max_src_in_batch = 0\n",
    "        max_tgt_in_batch = 0\n",
    "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
    "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
    "    src_elements = count * max_src_in_batch\n",
    "    tgt_elements = count * max_tgt_in_batch\n",
    "    return max(src_elements, tgt_elements)\n",
    "\n",
    "class MyIterator(data.Iterator):\n",
    "    def create_batches(self):\n",
    "        if self.train:\n",
    "            def pool(d, random_shuffler):\n",
    "                for p in data.batch(d, self.batch_size * 100):\n",
    "                    p_batch = data.batch(\n",
    "                        sorted(p, key=self.sort_key),\n",
    "                        self.batch_size, self.batch_size_fn)\n",
    "                    for b in random_shuffler(list(p_batch)):\n",
    "                        yield b\n",
    "            self.batches = pool(self.data(), self.random_shuffler)\n",
    "            \n",
    "        else:\n",
    "            self.batches = []\n",
    "            for b in data.batch(self.data(), self.batch_size,\n",
    "                                          self.batch_size_fn):\n",
    "                self.batches.append(sorted(b, key=self.sort_key))\n",
    "\n",
    "def rebatch(pad_idx, batch):\n",
    "    \"Fix order in torchtext to match ours\"\n",
    "    src, trg = batch.src.transpose(0, 1), batch.trg.transpose(0, 1)\n",
    "    src_mask, trg_mask = make_std_mask(src, trg, pad_idx)\n",
    "    return Batch(src, trg, src_mask, trg_mask, (trg[1:] != pad_idx).data.sum())\n",
    "\n",
    "train_iter = MyIterator(train, batch_size=BATCH_SIZE, device=0,\n",
    "                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
    "                        batch_size_fn=batch_size_fn, train=True)\n",
    "valid_iter = MyIterator(val, batch_size=BATCH_SIZE, device=0,\n",
    "                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
    "                        batch_size_fn=batch_size_fn, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1900
    },
    "colab_type": "code",
    "id": "wamR3SPdUgRo",
    "outputId": "b3b92f29-2560-4dbb-c2b0-f2a0b71db37c"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-b72405416f0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSRC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTGT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_std_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \"\"\"\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    150\u001b[0m                 \u001b[0;31m# Variables stored in modules are graph leaves, and we don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[0;31m# want to create copy nodes, so we have to unpack the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m                 \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \"\"\"\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_new\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_lazy_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m     \u001b[0;31m# We need this method only for lazy init, so we can remove it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0m_CudaBase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m         raise RuntimeError(\n\u001b[1;32m    119\u001b[0m             \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_sparse_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_check_driver\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mFound\u001b[0m \u001b[0mno\u001b[0m \u001b[0mNVIDIA\u001b[0m \u001b[0mdriver\u001b[0m \u001b[0mon\u001b[0m \u001b[0myour\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mPlease\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0mthat\u001b[0m \u001b[0myou\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mhave\u001b[0m \u001b[0man\u001b[0m \u001b[0mNVIDIA\u001b[0m \u001b[0mGPU\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minstalled\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdriver\u001b[0m \u001b[0;32mfrom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m http://www.nvidia.com/Download/index.aspx\"\"\")\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;31m# TODO: directly link to the alternative bin that needs install\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nFound no NVIDIA driver on your system. Please check that you\nhave an NVIDIA GPU and installed a driver from\nhttp://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "# Create the model an load it onto our GPU.\n",
    "pad_idx = TGT.vocab.stoi[\"<blank>\"]\n",
    "model = make_model(len(SRC.vocab), len(TGT.vocab), N=6)\n",
    "model_opt = get_std_opt(model)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SStCCZoiUgRp",
    "outputId": "a551e444-2fe5-4cd5-f46a-3de3505a2545"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 9.299771845340729 6.987712429686844e-07\n",
      "11 9.415135336574167 4.192627457812107e-06\n",
      "21 8.813630282878876 7.686483672655528e-06\n",
      "31 9.112178653478622 1.118033988749895e-05\n",
      "41 8.607461810112 1.4674196102342371e-05\n",
      "51 8.913826749660075 1.8168052317185794e-05\n",
      "61 8.701497752219439 2.1661908532029216e-05\n",
      "71 8.373274087905884 2.515576474687264e-05\n",
      "81 8.454237446188927 2.8649620961716057e-05\n",
      "91 7.6996782422065735 3.214347717655948e-05\n",
      "101 8.037408232688904 3.56373333914029e-05\n",
      "111 7.704962134361267 3.913118960624633e-05\n",
      "121 7.699015600606799 4.262504582108975e-05\n",
      "131 7.367554426193237 4.611890203593317e-05\n",
      "141 7.2071177661418915 4.961275825077659e-05\n",
      "151 7.106400920893066 5.310661446562001e-05\n",
      "161 6.804656069725752 5.660047068046343e-05\n",
      "171 6.390337720513344 6.0094326895306855e-05\n",
      "181 5.687528342008591 6.358818311015028e-05\n",
      "191 6.122820109128952 6.70820393249937e-05\n",
      "201 5.829070374369621 7.057589553983712e-05\n"
     ]
    }
   ],
   "source": [
    "\n",
    "criterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, smoothing=0.1)\n",
    "criterion.cuda()\n",
    "for epoch in range(15):\n",
    "    train_epoch((rebatch(pad_idx, b) for b in train_iter), model, criterion, model_opt)\n",
    "    valid_epoch((rebatch(pad_idx, b) for b in valid_iter), model, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1NO9lsw2UgRt"
   },
   "source": [
    "\n",
    "OTHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B8BVm-hEUgRw"
   },
   "outputs": [],
   "source": [
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "BLANK_WORD = \"<blank>\"\n",
    "SRC = data.Field()\n",
    "TGT = data.Field(init_token = BOS_WORD, eos_token = EOS_WORD, pad_token=BLANK_WORD) # only target needs BOS/EOS\n",
    "\n",
    "MAX_LEN = 100\n",
    "train = datasets.TranslationDataset(path=\"/n/home00/srush/Data/baseline-1M_train.tok.shuf\", \n",
    "                                    exts=('.en', '.fr'),\n",
    "                                    fields=(SRC, TGT), \n",
    "                                    filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "                                         len(vars(x)['trg']) <= MAX_LEN)\n",
    "SRC.build_vocab(train.src, max_size=50000)\n",
    "TGT.build_vocab(train.trg, max_size=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eFdZyOIzUgRx",
    "outputId": "bc543dba-c343-47bc-e81e-f74a25688668"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): EncoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm(\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): DecoderLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (src_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512)\n",
       "            (1): Linear(in_features=512, out_features=512)\n",
       "            (2): Linear(in_features=512, out_features=512)\n",
       "            (3): Linear(in_features=512, out_features=512)\n",
       "          )\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=512, out_features=2048)\n",
       "          (w_2): Linear(in_features=2048, out_features=512)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (1): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (2): SublayerConnection(\n",
       "            (norm): LayerNorm(\n",
       "            )\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm(\n",
       "    )\n",
       "  )\n",
       "  (src_embed): Sequential(\n",
       "    (0): Embeddings(\n",
       "      (lut): Embedding(50002, 512)\n",
       "    )\n",
       "    (1): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (tgt_embed): Sequential(\n",
       "    (0): Embeddings(\n",
       "      (lut): Embedding(50004, 512)\n",
       "    )\n",
       "    (1): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       "  (generator): Generator(\n",
       "    (proj): Linear(in_features=512, out_features=50004)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_idx = TGT.vocab.stoi[\"<blank>\"]\n",
    "print(pad_idx)\n",
    "model = make_model(len(SRC.vocab), len(TGT.vocab), pad_idx, N=6)\n",
    "model_opt = get_opt(model)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cinuTkbtUgRz"
   },
   "outputs": [],
   "source": [
    "criterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, label_smoothing=0.1)\n",
    "criterion.cuda()\n",
    "for epoch in range(15):\n",
    "    train_epoch(train_iter, model, criterion, model_opt)\n",
    "    valid_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PsoeJn4bUgR1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4LadFBIEUgR3",
    "outputId": "ba9a812f-b5f6-4972-af91-215d91a223d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "50002\n"
     ]
    }
   ],
   "source": [
    "print(pad_idx)\n",
    "print(len(SRC.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kP_Au0bHUgR7",
    "outputId": "d5ad5d88-d512-4947-b5b8-f18eaba4f9ff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type EncoderDecoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type EncoderLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type MultiHeadedAttention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type PositionwiseFeedForward. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type SublayerConnection. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type LayerNorm. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type Decoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type DecoderLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type Embeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type PositionalEncoding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type Generator. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, \"/n/rush_lab/trans_ipython.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nqKKIhoOUgR-",
    "outputId": "2c087978-9803-4639-886b-88df91e62096"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3.269582842476666 0.0005377044714644026\n",
      "101 3.300532897672383 0.0005726430336128369\n",
      "201 3.3047672072425485 0.0006075815957612711\n",
      "301 2.7151080842595547 0.0006425201579097052\n",
      "401 2.6975380268413574 0.0006774587200581396\n",
      "501 3.051631387323141 0.0007123972822065737\n",
      "601 2.554425454698503 0.000747335844355008\n",
      "701 2.6254820519825444 0.0007822744065034422\n",
      "801 2.868743653933052 0.0008172129686518764\n",
      "901 2.5978208642918617 0.0008521515308003106\n",
      "1001 2.5955790174775757 0.0008870900929487448\n",
      "1101 2.6764775353949517 0.000922028655097179\n",
      "1201 2.464000296778977 0.0009569672172456132\n",
      "1301 2.0503073083236814 0.0009919057793940475\n",
      "1401 2.295472824771423 0.0010268443415424816\n",
      "1501 2.245281406212598 0.0010617829036909158\n",
      "1601 2.2577588511630893 0.00109672146583935\n",
      "1701 2.2232908592559397 0.0011316600279877844\n",
      "1801 2.357596361427568 0.0011665985901362186\n",
      "1901 2.121352154412307 0.0012015371522846527\n",
      "2001 2.5742998471250758 0.001236475714433087\n",
      "2101 2.2518509055953473 0.0012714142765815214\n",
      "2201 2.2251326659170445 0.0013063528387299553\n",
      "2301 2.078994876006618 0.0013412914008783896\n",
      "2401 2.068276036065072 0.001376229963026824\n",
      "2501 2.31435151558253 0.0013907788851585368\n",
      "2601 1.9106871648691595 0.0013738752565588634\n",
      "2701 2.183084836578928 0.0013575733592730722\n",
      "2801 2.4668076275847852 0.0013418383196400342\n",
      "2901 1.963176985620521 0.0013266380295186675\n",
      "3001 2.2140520309330896 0.0013119428705609764\n",
      "3101 2.6989458349489723 0.0012977254713568687\n",
      "3201 2.1293521663174033 0.0012839604929174666\n",
      "3301 2.1402786187827587 0.0012706244386700126\n",
      "3401 2.041781216394156 0.0012576954857216498\n",
      "3501 2.051893091876991 0.0012451533346344698\n",
      "3601 1.5498304846696556 0.001232979075358713\n",
      "3701 2.763939742697403 0.001221155067309524\n",
      "3801 2.7611468499198963 0.0012096648318570434\n",
      "3901 1.7321470333263278 0.0011984929557393293\n",
      "4001 2.139603299088776 0.0011876250041103701\n",
      "4101 2.1966493157087825 0.0011770474421074978\n",
      "4201 2.0962203710805625 0.0011667475639689723\n",
      "4301 1.9717675620922819 0.0011567134288575545\n",
      "4401 2.097687987901736 0.0011469338026529508\n",
      "4501 1.9319786678534001 0.001137398105067946\n",
      "4601 1.8846281475271098 0.0011280963615221983\n",
      "4701 1.9817245414596982 0.0011190191592759865\n",
      "4801 1.7659185670199804 0.0011101576073853326\n",
      "4901 2.188665813198895 0.0011015033000912066\n",
      "5001 2.1391192222399695 0.0010930482833001135\n",
      "5101 1.8125874139368534 0.0010847850238522342\n",
      "5201 1.6616800595074892 0.0010767063813072288\n",
      "5301 1.6544548005331308 0.0010688055820075176\n",
      "5401 1.9542939933016896 0.0010610761952049212\n",
      "5501 2.218412609123334 0.0010535121110594244\n",
      "5601 1.838119359650591 0.001046107520339004\n",
      "5701 1.892627771012485 0.0010388568956672375\n",
      "5801 2.2462481096954434 0.0010317549741811346\n",
      "5901 1.4471426841337234 0.0010247967414755423\n",
      "6001 1.9312338004237972 0.0010179774167228303\n",
      "6101 1.7303275546291843 0.001011292438867507\n",
      "6201 1.8833909621462226 0.0010047374538051973\n",
      "6301 1.8943474531406537 0.0009983083024640838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home00/srush/.conda/envs/py3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1.5940000533591956 0.0009927515780513657\n",
      "101 1.7524283765815198 0.0009865483707369156\n",
      "201 1.900138527940726 0.0009804600111146078\n",
      "301 1.8419977760640904 0.0009744829985071481\n",
      "401 1.9621913449373096 0.0009686139798247046\n",
      "501 2.226916428655386 0.0009628497416600543\n",
      "601 1.7190162097394932 0.0009571872028951208\n",
      "701 1.8589106332874508 0.0009516234077802563\n",
      "801 1.8107321247807704 0.000946155519450957\n",
      "901 1.6531266793608665 0.0009407808138497059\n",
      "1001 1.4840005157748237 0.0009354966740233614\n",
      "1101 1.7578403616789728 0.0009303005847689719\n",
      "1201 1.3920216620899737 0.0009251901276031373\n",
      "1301 1.6626927084289491 0.0009201629760320567\n",
      "1401 1.7256765578058548 0.0009152168911012566\n",
      "1501 1.6049046433763579 0.0009103497172056578\n",
      "1601 1.6955451717367396 0.000905559378142174\n",
      "1701 1.6796367820352316 0.0009008438733884249\n",
      "1801 1.5794002648835885 0.0008962012745924116\n",
      "1901 1.9637197174597532 0.0008916297222591652\n",
      "2001 1.4656428614398465 0.0008871274226214399\n",
      "2101 1.567156056407839 0.0008826926446824871\n",
      "2201 1.542241255287081 0.0008783237174198395\n",
      "2301 1.690121710913445 0.0008740190271398465\n",
      "2401 1.357302049640566 0.0008697770149734477\n",
      "2501 1.9049871656461619 0.0008655961745043597\n",
      "2601 2.240402895025909 0.0008614750495214811\n",
      "2701 1.7940634173137369 0.0008574122318878972\n",
      "2801 1.7314323161263019 0.0008534063595194054\n",
      "2901 1.6064868164248765 0.0008494561144659686\n",
      "3001 1.7515187719254754 0.0008455602210899614\n",
      "3101 1.552100334316492 0.0008417174443354889\n",
      "3201 1.6221882179379463 0.0008379265880834463\n",
      "3301 1.5139061958470847 0.0008341864935873445\n",
      "3401 1.6668659402348567 0.0008304960379852562\n",
      "3501 2.1993618682026863 0.0008268541328835436\n",
      "3601 1.823760490231507 0.0008232597230083089\n",
      "3701 1.8189842144493014 0.0008197117849207771\n",
      "3801 1.689056838164106 0.0008162093257930558\n",
      "3901 1.5656833801185712 0.0008127513822409492\n",
      "4001 1.5621904337021988 0.0008093370192107105\n",
      "4101 1.4836799805052578 0.0008059653289168093\n",
      "4201 1.47899504378438 0.000802635429827976\n",
      "4301 1.6922758186701685 0.0007993464656989501\n",
      "4401 1.636858390578709 0.000796097604645519\n",
      "4501 1.5558803144613194 0.0007928880382605766\n",
      "4601 1.5102424336364493 0.00078971698076907\n",
      "4701 1.541241532890126 0.0007865836682198282\n",
      "4801 1.5931309935403988 0.000783487357712386\n",
      "4901 1.2315586884506047 0.0007804273266570247\n",
      "5001 1.527937745093368 0.0007774028720663579\n",
      "5101 1.31743333209306 0.0007744133098768835\n",
      "5201 1.5960889644484269 0.0007714579742990187\n",
      "5301 1.4181096099782735 0.0007685362171942096\n",
      "5401 1.4596448407392018 0.0007656474074777987\n",
      "5501 1.4594163084111642 0.0007627909305463981\n",
      "5601 1.62109798315214 0.0007599661877285873\n",
      "5701 1.586864550015889 0.0007571725957578231\n",
      "5801 1.5062829439411871 0.0007544095862665088\n",
      "5901 1.4292167258172412 0.0007516766053002225\n",
      "6001 1.4355267270930199 0.0007489731128511653\n",
      "6101 1.4162533966591582 0.0007462985824099354\n",
      "6201 1.6518787188415445 0.0007436525005347853\n",
      "6301 1.5916137372114463 0.0007410343664375577\n",
      "1 1.202994157327339 0.0007387531385993765\n",
      "101 1.4649722938484047 0.0007361862332058686\n",
      "201 1.1459896704182029 0.0007336459004644837\n",
      "301 1.417104929103516 0.0007311316850490442\n",
      "401 1.373963651509257 0.0007286431424819469\n",
      "501 1.6432027550181374 0.0007261798388040814\n",
      "601 1.4122836171882227 0.0007237413502569408\n",
      "701 1.6119428309611976 0.0007213272629763972\n",
      "801 1.5545603609643877 0.0007189371726976359\n",
      "901 1.5427279596333392 0.0007165706844707772\n",
      "1001 1.5437391183004365 0.0007142274123867243\n",
      "1101 1.9743895339342998 0.0007119069793128112\n",
      "1201 1.730805973522365 0.0007096090166378355\n",
      "1301 1.5635135210759472 0.0007073331640260875\n",
      "1401 1.206731209764257 0.000705079069180001\n",
      "1501 1.4495476994197816 0.0007028463876110714\n",
      "1601 1.2935033895773813 0.0007006347824187037\n",
      "1701 1.1734203454107046 0.000698443924076667\n",
      "1801 1.202259551268071 0.0006962734902268488\n",
      "1901 1.7874216835407424 0.0006941231654800159\n",
      "2001 1.5438914835685864 0.0006919926412233024\n",
      "2101 1.5168145569041371 0.000689881615434157\n",
      "2201 1.5306344364071265 0.0006877897925004977\n",
      "2301 1.5227781175635755 0.0006857168830468271\n",
      "2401 1.3308223116910085 0.0006836626037660786\n",
      "2501 1.4871021673316136 0.0006816266772569715\n",
      "2601 1.3415705130901188 0.0006796088318666611\n",
      "2701 1.2746119699440897 0.0006776088015384847\n",
      "2801 1.3439618053671438 0.0006756263256646049\n",
      "2901 1.3065503026737133 0.0006736611489433701\n",
      "3001 1.4918707825127058 0.0006717130212412112\n",
      "3101 1.4003087060991675 0.0006697816974589058\n",
      "3201 1.3473156996478792 0.0006678669374020495\n",
      "3301 1.3869949235959211 0.0006659685056555759\n",
      "3401 1.5086751837225165 0.000664086171462178\n",
      "3501 1.4735991460911464 0.00066221970860449\n",
      "3601 1.3997832712557283 0.0006603688952908887\n",
      "3701 1.5196008981074556 0.0006585335140447885\n",
      "3801 1.2834229312138632 0.0006567133515973014\n",
      "3901 1.3874705795169575 0.0006549081987831418\n",
      "4001 1.6422591609880328 0.0006531178504396635\n",
      "4101 1.305389653716702 0.0006513421053089143\n",
      "4201 1.5159487561322749 0.0006495807659426053\n",
      "4301 1.3981374967552256 0.0006478336386098913\n",
      "4401 1.7390631912276149 0.0006461005332078655\n",
      "4501 1.3604947600979358 0.0006443812631746732\n",
      "4601 1.7799529591429746 0.000642675645405156\n",
      "4701 1.3463407127128448 0.0006409835001689394\n",
      "4801 1.4632963918847963 0.0006393046510308797\n",
      "4901 1.1903231081087142 0.0006376389247737917\n",
      "5001 1.3287691511941375 0.0006359861513233783\n",
      "5101 1.3445309301023372 0.0006343461636752915\n",
      "5201 1.5431754024625661 0.0006327187978242499\n",
      "5301 1.3343850841192761 0.0006311038926951474\n",
      "5401 1.1768817943520844 0.0006295012900760858\n",
      "5501 1.6530805606771537 0.0006279108345532683\n",
      "5601 1.2646167293241888 0.000626332373447694\n",
      "5701 1.3651119051501155 0.000624765756753594\n",
      "5801 1.831987822048177 0.0006232108370785525\n",
      "5901 1.3451470380132378 0.0006216674695852594\n",
      "6001 1.5295006221767835 0.0006201355119348414\n",
      "6101 1.2796215488779126 0.0006186148242317232\n",
      "6201 1.3307579715619795 0.0006171052689699666\n",
      "6301 1.5296110774725094 0.0006156067109810445\n",
      "1 1.355640010209754 0.0006142969713181733\n",
      "101 1.3438594869803637 0.0006128187302418007\n",
      "201 1.3398014856502414 0.0006113511097561582\n",
      "301 1.2453488917089999 0.0006098939832926246\n",
      "401 1.74672898178801 0.0006084472263842588\n",
      "501 1.348103358541266 0.0006070107166211413\n",
      "601 1.2492765338683967 0.0006055843336068713\n",
      "701 1.568915182055207 0.0006041679589161831\n",
      "801 1.3617599749704823 0.0006027614760536461\n",
      "901 1.3296397840604186 0.0006013647704134199\n",
      "1001 1.506301498040557 0.0005999777292400283\n",
      "1101 1.1846984136500396 0.000598600241590126\n",
      "1201 1.1235853107646108 0.0005972321982952243\n",
      "1301 1.3506322290195385 0.0005958734919253515\n",
      "1401 1.5431637589354068 0.0005945240167536175\n",
      "1501 1.4227895765798166 0.0005931836687216574\n",
      "1601 1.2444980588334147 0.0005918523454059284\n",
      "1701 1.37204463215312 0.000590529945984835\n",
      "1801 1.3662666375166737 0.0005892163712066582\n",
      "1901 1.758998476434499 0.0005879115233582672\n",
      "2001 1.3996043455335894 0.0005866153062345879\n",
      "2101 1.409632071852684 0.0005853276251088103\n",
      "2201 1.3139934270293452 0.0005840483867033116\n",
      "2301 1.2863777373568155 0.0005827774991612753\n",
      "2401 1.1966209802776575 0.0005815148720189864\n",
      "2501 1.3174833165830933 0.0005802604161787846\n",
      "2601 1.406668136944063 0.0005790140438826557\n",
      "2701 1.31760111481708 0.0005777756686864456\n",
      "2801 1.22686495014932 0.0005765452054346768\n",
      "2901 1.4871160766715548 0.0005753225702359537\n",
      "3001 1.3321835576352896 0.0005741076804389384\n",
      "3101 1.349290698301047 0.0005729004546088824\n",
      "3201 1.0498975263908505 0.0005717008125046992\n",
      "3301 1.4295434548403136 0.0005705086750565621\n",
      "3401 1.3862976277887356 0.0005693239643440145\n",
      "3501 1.3612052928074263 0.0005681466035745775\n",
      "3601 1.3539716337691061 0.0005669765170628427\n",
      "3701 1.3053378225304186 0.0005658136302100359\n",
      "3801 1.2067344364186283 0.0005646578694840415\n",
      "3901 1.417662046442274 0.0005635091623998715\n",
      "4001 1.2578378450434684 0.0005623674375005725\n",
      "4101 1.2363171717152 0.0005612326243385544\n",
      "4201 1.3426340871083084 0.0005601046534573332\n",
      "4301 1.3097076122212457 0.000558983456373675\n",
      "4401 1.0131576862186193 0.0005578689655601316\n",
      "4501 1.4332989812392043 0.000556761114427959\n",
      "4601 1.4043821960221976 0.0005556598373104054\n",
      "4701 1.373746110650245 0.0005545650694463629\n",
      "4801 1.2657524709356949 0.0005534767469643717\n",
      "4901 1.1224889098666608 0.0005523948068669684\n",
      "5001 1.2615516305086203 0.000551319187015369\n",
      "5101 1.409785834257491 0.0005502498261144795\n",
      "5201 1.3791224808810512 0.0005491866636982242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5301 1.2408291140163783 0.0005481296401151859\n",
      "5401 1.3008261130889878 0.0005470786965145471\n",
      "5501 1.1700160388209042 0.0005460337748323287\n",
      "5601 1.2999350049067289 0.000544994817777915\n",
      "5701 1.3322585223941132 0.0005439617688208604\n",
      "5801 1.254337038320955 0.0005429345721779703\n",
      "5901 1.773689029644629 0.000541913172800649\n",
      "6001 1.3898115772462916 0.0005408975163625087\n",
      "6101 1.4735579792177305 0.0005398875492472326\n",
      "6201 1.05738219874911 0.000538883218536687\n",
      "6301 1.0802461032290012 0.0005378844719992749\n",
      "1 1.286231731530279 0.0005370101533168812\n",
      "101 1.2250633136718534 0.0005360217659787991\n",
      "201 1.239320948603563 0.0005350388161199592\n",
      "301 1.4140636462761904 0.0005340612540665886\n",
      "401 1.442663955502212 0.0005330890307779102\n",
      "501 1.4505203103472013 0.0005321220978358095\n",
      "601 1.2115196966333315 0.0005311604074347066\n",
      "701 1.2035035027656704 0.0005302039123716286\n",
      "801 1.3747974793659523 0.0005292525660364788\n",
      "901 1.36490419106849 0.0005283063224024965\n",
      "1001 1.1864821948111057 0.0005273651360169036\n",
      "1101 1.1623371304303873 0.000526428961991735\n",
      "1201 1.1043747729854658 0.0005254977559948457\n",
      "1301 1.6982813560443901 0.0005245714742410941\n",
      "1401 1.2719842366641387 0.0005236500734836944\n",
      "1501 1.2951120301149786 0.0005227335110057353\n",
      "1601 1.580276207998395 0.0005218217446118628\n",
      "1701 1.218743062199792 0.0005209147326201215\n",
      "1801 1.1479590674862266 0.0005200124338539494\n",
      "1901 1.2872504810075043 0.0005191148076343284\n",
      "2001 1.8993003838438653 0.0005182218137720798\n",
      "2101 1.2762204335303977 0.0005173334125603075\n",
      "2201 1.6183682525045242 0.0005164495647669814\n",
      "2301 1.2522982619411778 0.0005155702316276618\n",
      "2401 1.2925108795752749 0.0005146953748383575\n",
      "2501 1.340747339767404 0.0005138249565485178\n",
      "2601 1.340512964350637 0.0005129589393541545\n",
      "2701 1.1672844442073256 0.0005120972862910908\n",
      "2801 1.257948145037517 0.0005112399608283344\n",
      "2901 1.510728154462413 0.0005103869268615725\n",
      "3001 1.4130934766726568 0.0005095381487067851\n",
      "3101 1.2367545471934136 0.0005086935910939762\n",
      "3201 1.3846962348325178 0.0005078532191610173\n",
      "3301 1.2582954101526411 0.0005070169984476032\n",
      "3401 1.1545094328466803 0.0005061848948893172\n",
      "3501 1.295005505089648 0.0005053568748118022\n",
      "3601 1.3319955187034793 0.0005045329049250373\n",
      "3701 1.3548947679810226 0.000503712952317716\n",
      "3801 1.4635376840888057 0.0005028969844517252\n",
      "3901 1.6542128916307774 0.0005020849691567213\n",
      "4001 1.3512894048908493 0.0005012768746248036\n",
      "4101 1.397591198408918 0.0005004726694052806\n",
      "4201 1.3055676214280538 0.0004996723223995292\n",
      "4301 1.3375271083787084 0.000498875802855943\n",
      "4401 1.2366086341207847 0.0004980830803649704\n",
      "4501 1.2439679206581786 0.0004972941248542376\n",
      "4601 1.352382222772576 0.0004965089065837576\n",
      "4701 1.7570512742054234 0.0004957273961412208\n",
      "4801 1.232903058291413 0.0004949495644373684\n",
      "4901 1.015858386293985 0.0004941753827014446\n",
      "5001 1.381107110035373 0.000493404822476726\n",
      "5101 0.9564947709441185 0.0004926378556161293\n",
      "5201 1.228621664486127 0.0004918744542778926\n",
      "5301 1.182083563413471 0.0004911145909213302\n",
      "5401 1.2583643229590962 0.0004903582383026592\n",
      "5501 1.404046923678834 0.0004896053694708976\n",
      "5601 1.2389367091745953 0.0004888559577638302\n",
      "5701 1.119320425321348 0.00048810997680404295\n",
      "5801 1.586507015679672 0.0004873674004950231\n",
      "5901 1.112720330056618 0.0004866282030173253\n",
      "6001 1.3577893248293549 0.0004858923588248005\n",
      "6101 1.217524498468265 0.0004851598426408882\n",
      "6201 1.3229771983387764 0.0004844306294549693\n",
      "6301 1.5693217546272535 0.0004837046945187796\n",
      "1 1.1786362157727126 0.00048306134975017534\n",
      "101 1.28241519164294 0.00048234154403106603\n",
      "201 1.1411214591062162 0.00048162494648183897\n",
      "301 1.2352831599419005 0.0004809115333417623\n",
      "401 1.1032181181944907 0.00048020128109574806\n",
      "501 1.18390864826506 0.00047949416647109663\n",
      "601 1.2226583541632863 0.00047879016643429347\n",
      "701 1.0373018080717884 0.0004780892581878584\n",
      "801 1.2819566036341712 0.00047739141916724456\n",
      "901 1.1648676298791543 0.0004766966270377871\n",
      "1001 1.1654199322802015 0.00047600485969170105\n",
      "1101 1.2386636545270449 0.00047531609524512704\n",
      "1201 1.2253044219105504 0.0004746303120352227\n",
      "1301 1.375744077755371 0.0004739474886173019\n",
      "1401 1.1551300736318808 0.0004732676037620178\n",
      "1501 1.5255512128351256 0.00047259063645259034\n",
      "1601 1.255034319277911 0.00047191656588207824\n",
      "1701 1.1623500876303297 0.0004712453714506923\n",
      "1801 1.2958592986833537 0.00047057703276315175\n",
      "1901 1.1341320046922192 0.0004699115296260807\n",
      "2001 1.1937441515619867 0.0004692488420454462\n",
      "2101 1.7062073841661913 0.00046858895022403485\n",
      "2201 1.2566360468044877 0.00046793183455896863\n",
      "2301 1.2216275975806639 0.0004672774756392595\n",
      "2401 1.2636524712725077 0.0004666258542434008\n",
      "2501 1.2113699619076215 0.00046597695133699556\n",
      "2601 1.1559934263350442 0.00046533074807042176\n",
      "2701 1.256740387296304 0.0004646872257765319\n",
      "2801 1.3039579528664262 0.0004640463659683885\n",
      "2901 1.2651300196012016 0.0004634081503370334\n",
      "3001 1.2652980692801066 0.000462772560749291\n",
      "3101 1.1218284339411184 0.00046213957924560355\n",
      "3201 1.2543016897689085 0.0004615091880379007\n",
      "3301 1.2131407480192138 0.0004608813695074994\n",
      "3401 1.2994702684518415 0.0004602561062030357\n",
      "3501 1.2115506358095445 0.00045963338083842724\n",
      "3601 1.1760960748360958 0.00045901317629086643\n",
      "3701 1.0682971130590886 0.00045839547559884254\n",
      "3801 1.0764332090620883 0.00045778026196019347\n",
      "3901 1.1835216325707734 0.0004571675187301866\n",
      "4001 1.3529939632862806 0.00045655722941962654\n",
      "4101 1.3684578015236184 0.0004559493776929923\n",
      "4201 1.2233722301607486 0.0004553439473666001\n",
      "4301 1.2596116681525018 0.0004547409224067939\n",
      "4401 1.2757911044172943 0.00045414028692816196\n",
      "4501 1.2199301174841821 0.0004535420251917793\n",
      "4601 1.3471774608151463 0.0004529461216034753\n",
      "4701 1.475795219448628 0.0004523525607121267\n",
      "4801 1.1835241899825633 0.0004517613272079745\n",
      "4901 1.1791616377497576 0.0004511724059209659\n",
      "5001 1.3126113665202865 0.0004505857818191191\n",
      "5101 1.2516068609402282 0.0004500014400069121\n",
      "5201 1.178165558274486 0.0004494193657236937\n",
      "5301 1.6013869942435122 0.0004488395443421177\n",
      "5401 1.2677101592962572 0.00044826196136659916\n",
      "5501 1.1976667390699731 0.0004476866024317922\n",
      "5601 1.1990807302790927 0.00044711345330108884\n",
      "5701 1.1415361673789448 0.0004465424998651406\n",
      "5801 1.2389779405202717 0.0004459737281403985\n",
      "5901 1.1746156329172663 0.0004454071242676752\n",
      "6001 1.1718775559565984 0.0004448426745107265\n",
      "6101 1.1669323876558337 0.00044428036525485275\n",
      "6201 1.22836275130976 0.0004437201830055194\n",
      "6301 1.1068585112225264 0.000443162114386997\n",
      "1 1.1908240653865505 0.00044267275186678196\n",
      "101 1.156728027795907 0.0004421186210736662\n",
      "201 1.151486962888157 0.0004415665660409348\n",
      "301 1.1075408830074593 0.0004410165738412884\n",
      "401 1.1251853418070823 0.00044046863165985925\n",
      "501 1.224421168473782 0.0004399227267929559\n",
      "601 1.1097798637929372 0.00043937884664682695\n",
      "701 0.992531725903973 0.0004388369787364407\n",
      "801 1.2762772621936165 0.0004382971106842813\n",
      "901 1.154728337773122 0.00043775923021916087\n",
      "1001 0.9699444866273552 0.00043722332517504866\n",
      "1101 1.1039727496681735 0.0004366893834899152\n",
      "1201 1.2997219555545598 0.0004361573932045913\n",
      "1301 1.5713044246076606 0.00043562734246164385\n",
      "1401 1.1782071397465188 0.00043509921950426545\n",
      "1501 1.256332863289117 0.0004345730126751789\n",
      "1601 1.162631830346072 0.00043404871041555687\n",
      "1701 1.1123517343075946 0.00043352630126395546\n",
      "1801 1.0946980192093179 0.0004330057738552615\n",
      "1901 1.120711475959979 0.0004324871169196544\n",
      "2001 1.1385652619646862 0.0004319703192815812\n",
      "2101 1.0391206528292969 0.0004314553698587452\n",
      "2201 1.1468603002722375 0.00043094225766110786\n",
      "2301 1.1944863148819422 0.0004304309717899036\n",
      "2401 1.1445480604888871 0.00042992150143666746\n",
      "2501 1.160092411795631 0.0004294138358822756\n",
      "2601 0.905779943568632 0.00042890796449599795\n",
      "2701 1.2337692737637553 0.0004284038767345632\n",
      "2801 1.2654334787439439 0.00042790156214123586\n",
      "2901 1.2613030684588011 0.0004274010103449054\n",
      "3001 1.1566388571663992 0.0004269022110591865\n",
      "3101 1.1506170178181492 0.0004264051540815317\n",
      "3201 1.1042177192866802 0.00042590982929235444\n",
      "3301 1.268968387885252 0.00042541622665416415\n",
      "3401 1.1708880871301517 0.0004249243362107117\n",
      "3501 1.1094016103306785 0.00042443414808614573\n",
      "3601 1.3188527839665767 0.0004239456524841804\n",
      "3701 1.2144307589042 0.0004234588396872726\n",
      "3801 1.1827894128946355 0.0004229737000558104\n",
      "3901 0.9924444004427642 0.00042249022402731095\n",
      "4001 1.1228576390712988 0.0004220084021156294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4101 1.1924936635477934 0.0004215282249101765\n",
      "4201 1.1275967326655518 0.0004210496830751471\n",
      "4301 1.0625419117277488 0.0004205727673487576\n",
      "4401 1.1389823842037003 0.00042009746854249313\n",
      "4501 1.339291847194545 0.00041962377754036395\n",
      "4601 1.0302886090357788 0.0004191516852981713\n",
      "4701 1.7122778899138211 0.00041868118284278167\n",
      "4801 1.2910672437865287 0.0004182122612714111\n",
      "4901 1.0494152382598259 0.00041774491175091685\n",
      "5001 1.1782474033534527 0.0004172791255170995\n",
      "5101 1.040663594380021 0.0004168148938740118\n",
      "5201 0.9901199785526842 0.00041635220819327733\n",
      "5301 1.5490817801910453 0.00041589105991341656\n",
      "5401 1.1346296942792833 0.00041543144053918197\n",
      "5501 1.223581779631786 0.00041497334164089994\n",
      "5601 0.958975835936144 0.0004145167548538224\n",
      "5701 1.238148811913561 0.0004140616718774844\n",
      "5801 1.1881117207813077 0.000413608084475071\n",
      "5901 1.1295715225860476 0.0004131559844727907\n",
      "6001 1.0610488005331717 0.000412705363759257\n",
      "6101 1.2371235750615597 0.00041225621428487707\n",
      "6201 1.1479767516138963 0.00041180852806124783\n",
      "6301 1.2059640220250003 0.0004113622971605593\n",
      "1 1.1765662879188312 0.0004109663692823915\n",
      "101 1.219779463717714 0.0004105228675028437\n",
      "201 0.972488499362953 0.00041008079847008953\n",
      "301 1.1617506150214467 0.0004096401544864815\n",
      "401 1.0883347367926035 0.0004092009279121472\n",
      "501 1.1085488446406089 0.00040876311116443343\n",
      "601 1.1023443718672752 0.0004083266967173559\n",
      "701 1.018611608800711 0.00040789167710105623\n",
      "801 1.1658196483003849 0.00040745804490126497\n",
      "901 1.2917855954219704 0.0004070257927587706\n",
      "1001 1.186474629881559 0.00040659491336889525\n",
      "1101 1.127356821874855 0.00040616539948097586\n",
      "1201 1.1975307842949405 0.00040573724389785204\n",
      "1301 1.1174790017685154 0.0004053104394753595\n",
      "1401 1.0863252188792103 0.0004048849791218294\n",
      "1501 1.0622602235816885 0.000404460855797593\n",
      "1601 1.1195327076129615 0.00040403806251449327\n",
      "1701 1.2447982146404684 0.00040361659233540054\n",
      "1801 1.2607627244724426 0.0004031964383737348\n",
      "1901 1.278331945562968 0.00040277759379299307\n",
      "2001 1.025594950420782 0.0004023600518062819\n",
      "2101 1.115274733179831 0.0004019438056758561\n",
      "2201 1.1167924739420414 0.0004015288487126612\n",
      "2301 1.0995151306560729 0.000401115174275883\n",
      "2401 1.1547567544039339 0.00040070277577250023\n",
      "2501 1.1590442548913416 0.00040029164665684384\n",
      "2601 1.1047132272506133 0.00039988178043016053\n",
      "2701 1.0620037270709872 0.00039947317064018093\n",
      "2801 1.0939110746548977 0.00039906581088069363\n",
      "2901 0.9693534299731255 0.0003986596947911227\n",
      "3001 1.2754340882529505 0.0003982548160561108\n",
      "3101 2.0011723663365046 0.0003978511684051071\n",
      "3201 1.1672507325711194 0.0003974487456119586\n",
      "3301 0.8547125565819442 0.00039704754149450736\n",
      "3401 1.0948779656609986 0.00039664754991419163\n",
      "3501 1.1662541554399013 0.0003962487647756509\n",
      "3601 1.242357063729287 0.00039585118002633614\n",
      "3701 1.142975198366912 0.000395454789656124\n",
      "3801 1.2055247909738682 0.0003950595876969351\n",
      "3901 1.2129587295930833 0.0003946655682223565\n",
      "4001 1.239052205113694 0.0003942727253472687\n",
      "4101 1.1285374546132516 0.0003938810532274764\n",
      "4201 1.3123125492420513 0.00039349054605934306\n",
      "4301 1.2088057449145708 0.00039310119807943006\n",
      "4401 1.0897887839237228 0.00039271300356413926\n",
      "4501 1.2131327866227366 0.00039232595682935973\n",
      "4601 0.9877158605959266 0.000391940052230118\n",
      "4701 1.2145014504967548 0.0003915552841602323\n",
      "4801 1.1513765653944574 0.0003911716470519708\n",
      "4901 1.1751896993955597 0.0003907891353757127\n",
      "5001 1.1771067604749987 0.0003904077436396139\n",
      "5101 1.3224336538114585 0.0003900274663892758\n",
      "5201 1.355893952131737 0.0003896482982074174\n",
      "5301 1.1996791153214872 0.0003892702337135512\n",
      "5401 1.206806231304654 0.0003888932675636631\n",
      "5501 1.280991047504358 0.0003885173944498942\n",
      "5601 0.9168080711970106 0.0003881426091002278\n",
      "5701 1.1924245768459514 0.0003877689062781782\n",
      "5801 1.1940782586170826 0.0003873962807824836\n",
      "5901 1.2784329915011767 0.0003870247274468023\n",
      "6001 1.1448089724872261 0.00038665424113941134\n",
      "6101 1.1181278676813236 0.0003862848167629092\n",
      "6201 1.2686003018752672 0.00038591644925392126\n",
      "6301 1.0795898175565526 0.000385549133582808\n",
      "1 1.199639980099164 0.00038523407955521927\n",
      "101 1.0967486363369972 0.00038486870703897236\n",
      "201 1.2039306252845563 0.00038450437215947677\n",
      "301 1.106695241353009 0.0003841410700146326\n",
      "401 1.0133273621067929 0.00038377879573470126\n",
      "501 1.1209047999582253 0.0003834175444820315\n",
      "601 1.073817516444251 0.00038305731145078797\n",
      "701 1.1002086726948619 0.00038269809186668256\n",
      "801 1.044247637852095 0.00038233988098670897\n",
      "901 1.1538543488713913 0.00038198267409887953\n",
      "1001 1.1638364950194955 0.00038162646652196454\n",
      "1101 1.1222136826545466 0.00038127125360523515\n",
      "1201 1.0936923912668135 0.0003809170307282081\n",
      "1301 1.0790816302178428 0.0003805637933003932\n",
      "1401 1.0973517978854943 0.0003802115367610436\n",
      "1501 1.3543376340385294 0.00037986025657890806\n",
      "1601 1.0638599513913505 0.0003795099482519871\n",
      "1701 1.095864930888638 0.0003791606073072896\n",
      "1801 1.3785420355270617 0.00037881222930059356\n",
      "1901 1.0656100183841772 0.0003784648098162084\n",
      "2001 1.083574770949781 0.0003781183444667399\n",
      "2101 1.7192173111798184 0.0003777728288928577\n",
      "2201 1.090653446619399 0.0003774282587630644\n",
      "2301 1.1122783308528597 0.00037708462977346826\n",
      "2401 0.95696423901245 0.0003767419376475568\n",
      "2501 1.2160079335735645 0.0003764001781359734\n",
      "2601 1.2086039673304185 0.00037605934701629616\n",
      "2701 1.0832101813866757 0.00037571944009281874\n",
      "2801 1.013074157119263 0.00037538045319633314\n",
      "2901 1.0823434699559584 0.00037504238218391556\n",
      "3001 1.1612105248786975 0.0003747052229387128\n",
      "3101 0.9126896761590615 0.0003743689713697328\n",
      "3201 1.3341417479550728 0.00037403362341163505\n",
      "3301 0.9600269035436213 0.0003736991750245252\n",
      "3401 1.1916928359714802 0.0003733656221937497\n",
      "3501 1.4976106537505984 0.0003730329609296942\n",
      "3601 1.1840611910447478 0.0003727011872675824\n",
      "3701 1.3150727476167958 0.0003723702972672783\n",
      "3801 1.221188339870423 0.00037204028701308904\n",
      "3901 1.2026138188084587 0.0003717111526135708\n",
      "4001 1.3793403076779214 0.00037138289020133557\n",
      "4101 1.0772470782976598 0.0003710554959328607\n",
      "4201 1.0168679640773917 0.0003707289659882998\n",
      "4301 1.1685322925950459 0.00037040329657129513\n",
      "4401 1.1224966624286026 0.00037007848390879306\n",
      "4501 1.0253048577578738 0.00036975452425085955\n",
      "4601 1.2101853350850433 0.00036943141387049916\n",
      "4701 1.050108958443161 0.0003691091490634741\n",
      "4801 1.2717001468117815 0.00036878772614812674\n",
      "4901 1.1231291381409392 0.00036846714146520227\n",
      "5001 1.1141781120968517 0.00036814739137767423\n",
      "5101 0.9994667179416865 0.00036782847227057074\n",
      "5201 1.3557415267750912 0.0003675103805508032\n",
      "5301 1.504937146051816 0.0003671931126469962\n",
      "5401 1.0444834220979828 0.00036687666500931896\n",
      "5501 1.0159150707913795 0.0003665610341093186\n",
      "5601 1.4691102042561397 0.00036624621643975515\n",
      "5701 1.2679062836105004 0.0003659322085144373\n",
      "5801 1.1070539963402553 0.0003656190068680607\n",
      "5901 1.2043958652066067 0.0003653066080560474\n",
      "6001 1.1217296464601532 0.00036499500865438625\n",
      "6101 1.2132740695233224 0.00036468420525947586\n",
      "6201 1.452793362134571 0.00036437419448796804\n",
      "6301 1.0731251265387982 0.00036406497297661317\n",
      "1 1.1998733360724145 0.00036379350826718935\n",
      "101 1.1498671763110906 0.0003634857615296514\n",
      "201 1.1022596344992053 0.00036317879447701637\n",
      "301 1.0011312331771478 0.00036287260382257964\n",
      "401 1.0373523531015962 0.0003625671862990008\n",
      "501 1.0644397677097004 0.000362262538658157\n",
      "601 1.1183270415349398 0.000361958657670998\n",
      "701 0.9439565273642074 0.00036165554012740277\n",
      "801 1.0483181119780056 0.0003613531828360362\n",
      "901 1.10791165966657 0.00036105158262420917\n",
      "1001 0.9895736404578201 0.00036075073633773743\n",
      "1101 1.0942751504917396 0.00036045064084080426\n",
      "1201 1.0274720180314034 0.0003601512930158222\n",
      "1301 1.049829078532639 0.0003598526897632977\n",
      "1401 1.0599873741302872 0.00035955482800169595\n",
      "1501 1.1110646773013286 0.00035925770466730756\n",
      "1601 1.1195740707335062 0.0003589613167141159\n",
      "1701 1.1175601889844984 0.0003586656611136664\n",
      "1801 1.27650167158572 0.00035837073485493607\n",
      "1901 1.1153064398095012 0.000358076534944205\n",
      "2001 1.4756392514391337 0.000357783058404929\n",
      "2101 0.8967400579713285 0.0003574903022776121\n",
      "2201 1.0554919667192735 0.0003571982636196827\n",
      "2301 1.1484477042686194 0.000356906939505368\n",
      "2401 1.0967925000586547 0.00035661632702557175\n",
      "2501 1.275310180048109 0.0003563264232877516\n",
      "2601 1.084061863599345 0.00035603722541579873\n",
      "2701 1.076280384673737 0.00035574873054991784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2801 1.200010517553892 0.0003554609358465082\n",
      "2901 1.059172057226533 0.000355173838478046\n",
      "3001 1.0261963941156864 0.000354887435632968\n",
      "3101 0.9737269952893257 0.0003546017245155551\n",
      "3201 1.109491402952699 0.0003543167023458187\n",
      "3301 1.0379895093501545 0.0003540323663593864\n",
      "3401 1.0210047286818735 0.00035374871380738974\n",
      "3501 1.3062616163679195 0.0003534657419563522\n",
      "3601 1.1297821700572968 0.00035318344808807914\n",
      "3701 1.095454223890556 0.0003529018294995477\n",
      "3801 1.0263875699602067 0.00035262088350279793\n",
      "3901 1.0200049103004858 0.00035234060742482575\n",
      "4001 1.139240143907955 0.00035206099860747537\n",
      "4101 1.185085133272878 0.00035178205440733397\n",
      "4201 1.0887831579057092 0.0003515037721956263\n",
      "4301 1.7533796445081862 0.000351226149358111\n",
      "4401 1.1149956742010545 0.00035094918329497705\n",
      "4501 1.1544227619015146 0.0003506728714207421\n",
      "4601 1.1121961249154992 0.00035039721116415036\n",
      "4701 1.0929120010696352 0.0003501221999680728\n",
      "4801 1.118996370700188 0.000349847835289407\n",
      "4901 0.9860638748505153 0.00034957411459897886\n",
      "5001 1.004449057742022 0.0003493010353814441\n",
      "5101 1.2927988782757893 0.00034902859513519165\n",
      "5201 0.98420900356723 0.0003487567913722472\n",
      "5301 1.1210692654858576 0.000348485621618178\n",
      "5401 1.163566045666812 0.00034821508341199766\n",
      "5501 1.17701395630138 0.0003479451743060731\n",
      "5601 1.0291424575298151 0.00034767589186603104\n",
      "5701 1.2543358486509533 0.0003474072336706659\n",
      "5801 1.2299754508348997 0.00034713919731184855\n",
      "5901 1.1936145080917413 0.0003468717803944353\n",
      "6001 0.9138605990447104 0.00034660498053617827\n",
      "6101 1.1037570714397589 0.00034633879536763624\n",
      "6201 1.04157008238235 0.00034607322253208626\n",
      "6301 1.1919174637878314 0.0003458082596854357\n",
      "1 0.9797578346915543 0.00034557559511139293\n",
      "101 0.9891712895187084 0.00034531177274179953\n",
      "201 1.1815550197497942 0.00034504855367990236\n",
      "301 1.1358435613219626 0.0003447859356297997\n",
      "401 1.0717519058380276 0.000344523916307803\n",
      "501 1.172375235328218 0.00034426249344235384\n",
      "601 1.0603420100815129 0.00034400166477394084\n",
      "701 1.0992282917286502 0.000343741428055018\n",
      "801 1.04559408465866 0.0003434817810499231\n",
      "901 1.0248865495998416 0.0003432227215347973\n",
      "1001 1.0598302248690743 0.0003429642472975047\n",
      "1101 1.0938185814011376 0.0003427063561375535\n",
      "1201 1.291374852447916 0.000342449045866017\n",
      "1301 1.0285806620959193 0.0003421923143054557\n",
      "1401 1.17992360109929 0.0003419361592898398\n",
      "1501 0.9615428688703105 0.0003416805786644727\n",
      "1601 1.2486475716141285 0.0003414255702859144\n",
      "1701 0.9794061238644645 0.0003411711320219065\n",
      "1801 1.1059001302346587 0.00034091726175129706\n",
      "1901 0.9131126408465207 0.00034066395736396637\n",
      "2001 0.9930663524428383 0.0003404112167607534\n",
      "2101 1.0812218267819844 0.0003401590378533823\n",
      "2201 1.0715046060213353 0.0003399074185643907\n",
      "2301 1.1185214965953492 0.00033965635682705713\n",
      "2401 1.05721441534115 0.00033940585058533\n",
      "2501 1.0936700437378022 0.00033915589779375693\n",
      "2601 1.07034515045234 0.00033890649641741454\n",
      "2701 1.0813248989579733 0.00033865764443183875\n",
      "2801 1.0510470166627783 0.0003384093398229561\n",
      "2901 0.9623011860530823 0.0003381615805870148\n",
      "3001 1.1494725269731134 0.00033791436473051725\n",
      "3101 1.2335324875239166 0.0003376676902701525\n",
      "3201 1.0398004396120086 0.00033742155523272933\n",
      "3301 1.0589452146668918 0.0003371759576551101\n",
      "3401 1.084106142167002 0.00033693089558414497\n",
      "3501 1.0406659920408856 0.0003366863670766065\n",
      "3601 0.9325393754988909 0.00033644237019912526\n",
      "3701 1.0507557194505353 0.0003361989030281253\n",
      "3801 0.945562198292464 0.0003359559636497606\n",
      "3901 1.0489263081690297 0.0003357135501598519\n",
      "4001 1.0528855019947514 0.00033547166066382383\n",
      "4101 1.1952165458351374 0.0003352302932766432\n",
      "4201 1.0958911241032183 0.00033498944612275674\n",
      "4301 1.077680416405201 0.0003347491173360301\n",
      "4401 1.2972540742484853 0.0003345093050596873\n",
      "4501 1.039087069220841 0.0003342700074462501\n",
      "4601 0.9597453814931214 0.00033403122265747876\n",
      "4701 1.1728105103829876 0.00033379294886431207\n",
      "4801 1.1258336059836438 0.0003335551842468092\n",
      "4901 1.0011352995352354 0.0003333179269940906\n",
      "5001 0.9672558718448272 0.00033308117530428074\n",
      "5101 1.0522874586749822 0.0003328449273844502\n",
      "5201 1.0063609674107283 0.0003326091814505589\n",
      "5301 1.0480196221014921 0.00033237393572739917\n",
      "5401 1.0445173801199417 0.00033213918844854004\n",
      "5501 1.417743748796056 0.00033190493785627127\n",
      "5601 1.0902981872641249 0.000331671182201548\n",
      "5701 1.0301871165866032 0.00033143791974393625\n",
      "5801 1.4090074983541854 0.00033120514875155805\n",
      "5901 1.1904211936052889 0.0003309728675010378\n",
      "6001 1.1052953382313717 0.0003307410742774485\n",
      "6101 1.133972127106972 0.00033050976737425853\n",
      "6201 1.4820798086614104 0.00033027894509327907\n",
      "6301 1.1476092239608988 0.00033004860574461153\n",
      "1 1.0870586273958907 0.00032984630526377065\n",
      "101 1.00110832543578 0.00032961686928176145\n",
      "201 0.9876259700540686 0.0003293879114110055\n",
      "301 1.2413200094233616 0.0003291594299932822\n",
      "401 0.9424758276436478 0.00032893142337841173\n",
      "501 1.0926933737646323 0.00032870388992420444\n",
      "601 1.0706572374765528 0.0003284768279964114\n",
      "701 1.0879125816572923 0.00032825023596867546\n",
      "801 1.262531905740616 0.0003280241122224816\n",
      "901 1.050877535046311 0.0003277984551471088\n",
      "1001 1.135452825037646 0.0003275732631395822\n",
      "1101 1.0099836179433623 0.0003273485346046242\n",
      "1201 1.1641883124248125 0.0003271242679546084\n",
      "1301 1.0249766572378576 0.00032690046160951133\n",
      "1401 1.216345368164184 0.0003266771139968662\n",
      "1501 1.4009095890432945 0.00032645422355171653\n",
      "1601 0.9214687866624445 0.00032623178871657\n",
      "1701 1.050587208737852 0.0003260098079413526\n",
      "1801 1.0106142781150993 0.0003257882796833635\n",
      "1901 0.9388233295176178 0.00032556720240723\n",
      "2001 1.1458081254386343 0.0003253465745848626\n",
      "2101 1.0818304931126477 0.00032512639469541087\n",
      "2201 0.8635702040046453 0.0003249066612252194\n",
      "2301 1.0180434776411857 0.00032468737266778394\n",
      "2401 1.0467977939988486 0.0003244685275237081\n",
      "2501 1.083000476603047 0.0003242501243006605\n",
      "2601 1.1069668279960752 0.00032403216151333166\n",
      "2701 1.086160118225962 0.00032381463768339173\n",
      "2801 1.0924749624973629 0.0003235975513394485\n",
      "2901 1.009744831302669 0.00032338090101700554\n",
      "3001 0.9085385013604537 0.0003231646852584205\n",
      "3101 1.1706041378201917 0.00032294890261286426\n",
      "3201 1.032271361502353 0.00032273355163627964\n",
      "3301 1.2584509218577296 0.00032251863089134133\n",
      "3401 1.2436874122358859 0.000322304138947415\n",
      "3501 1.0451832013077365 0.0003220900743805179\n",
      "3601 1.0900762653473066 0.00032187643577327854\n",
      "3701 1.1192542002827395 0.00032166322171489793\n",
      "3801 1.00253647408681 0.0003214504308011099\n",
      "3901 1.2160937447333708 0.0003212380616341424\n",
      "4001 1.0416435159859248 0.0003210261128226793\n",
      "4101 1.3598752447869629 0.00032081458298182156\n",
      "4201 1.0555532689650136 0.0003206034707330495\n",
      "4301 1.1295962483854964 0.00032039277470418526\n",
      "4401 0.9410244208120275 0.0003201824935293548\n",
      "4501 0.8939700378105044 0.00031997262584895135\n",
      "4601 0.908640876179561 0.000319763170309598\n",
      "4701 1.128680162204546 0.0003195541255641112\n",
      "4801 1.0497526655672118 0.00031934549027146444\n",
      "4901 1.0289937005145475 0.0003191372630967521\n",
      "5001 0.9918764412868768 0.0003189294427111535\n",
      "5101 1.2255524442298338 0.0003187220277918973\n",
      "5201 1.3292681298672733 0.0003185150170222263\n",
      "5301 0.878005885053426 0.00031830840909136197\n",
      "5401 1.0740752452165907 0.00031810220269447\n",
      "5501 1.0994656120092259 0.00031789639653262544\n",
      "5601 1.159670107124839 0.0003176909893127784\n",
      "5701 0.8859041188843548 0.0003174859797477199\n",
      "5801 1.084522244927939 0.00031728136655604814\n",
      "5901 1.4824702723776682 0.0003170771484621346\n",
      "6001 1.230977819112013 0.0003168733241960908\n",
      "6101 1.0119965468620649 0.00031666989249373517\n",
      "6201 1.1002646164814678 0.00031646685209656003\n",
      "6301 1.1440792203939054 0.00031626420175169897\n",
      "1 1.0551144047021808 0.0003160882122689669\n",
      "101 1.0408390048833098 0.00031588628797931984\n",
      "201 1.0153360446565785 0.0003156847501772966\n",
      "301 1.01748421555385 0.0003154835976315582\n",
      "401 1.1267781729111448 0.0003152828291162512\n",
      "501 1.0327468327741371 0.0003150824434109757\n",
      "601 0.9960314880299848 0.0003148824393007546\n",
      "701 1.0631007702104398 0.00031468281557600267\n",
      "801 1.0372768385277595 0.00031448357103249544\n",
      "901 1.031023440795252 0.0003142847044713392\n",
      "1001 0.9323838343843818 0.0003140862146989404\n",
      "1101 0.850510573014617 0.0003138881005269756\n",
      "1201 1.0943628003296908 0.0003136903607723615\n",
      "1301 1.0844742289336864 0.00031349299425722566\n",
      "1401 1.4024500491796061 0.00031329599980887637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1501 1.1463393379817717 0.00031309937625977405\n",
      "1601 1.0650637014914537 0.0003129031224475018\n",
      "1701 1.1410465109511279 0.00031270723721473664\n",
      "1801 1.0148204645956866 0.0003125117194092209\n",
      "1901 0.9743651752360165 0.0003123165678837336\n",
      "2001 0.990701739974611 0.00031212178149606226\n",
      "2101 1.1128740338463103 0.00031192735910897496\n",
      "2201 1.5508626039809315 0.0003117332995901923\n",
      "2301 1.01486110695987 0.00031153960181235955\n",
      "2401 0.9611194784665713 0.0003113462646530196\n",
      "2501 1.0847897573257796 0.0003111532869945851\n",
      "2601 1.2803321699175285 0.00031096066772431187\n",
      "2701 1.0769891024538083 0.0003107684057342714\n",
      "2801 1.0039243546780199 0.00031057649992132457\n",
      "2901 1.0824949400266632 0.00031038494918709473\n",
      "3001 0.9644128995714709 0.00031019375243794144\n",
      "3101 0.9868561172188492 0.00031000290858493437\n",
      "3201 1.0903575613629073 0.00030981241654382685\n",
      "3301 1.2633273452374851 0.0003096222752350304\n",
      "3401 1.0088038056419464 0.000309432483583589\n",
      "3501 1.1047235757578164 0.0003092430405191533\n",
      "3601 1.163446888080216 0.00030905394497595545\n",
      "3701 1.018205283649877 0.00030886519589278384\n",
      "3801 1.0408570388099179 0.00030867679221295824\n",
      "3901 1.0657447287230752 0.00030848873288430483\n",
      "4001 0.930832964291767 0.0003083010168591314\n",
      "4101 1.4587874389944773 0.00030811364309420327\n",
      "4201 1.1227497690124437 0.0003079266105507184\n",
      "4301 1.1970081577601377 0.0003077399181942835\n",
      "4401 1.1083186157047749 0.00030755356499488986\n",
      "4501 1.0473160178516991 0.00030736754992688985\n",
      "4601 1.0928417469840497 0.0003071818719689727\n",
      "4701 1.0073067757184617 0.00030699653010414117\n",
      "4801 1.1523846584532293 0.00030681152331968824\n",
      "4901 1.1988015054084826 0.0003066268506071739\n",
      "5001 1.036811558995396 0.00030644251096240176\n",
      "5101 1.0675939484208357 0.0003062585033853964\n",
      "5201 1.0752534797684348 0.00030607482688038056\n",
      "5301 1.1183025861246279 0.0003058914804557523\n",
      "5401 1.0365500289481133 0.0003057084631240629\n",
      "5501 1.129350705537945 0.00030552577390199393\n",
      "5601 1.06671357084997 0.0003053434118103358\n",
      "5701 1.0859052878222428 0.0003051613758739652\n",
      "5801 1.0723684425465763 0.00030497966512182316\n",
      "5901 1.0603833084605867 0.0003047982785868937\n",
      "6001 1.0581669194652932 0.0003046172153061818\n",
      "6101 1.2675022858026068 0.0003044364743206923\n",
      "6201 1.0536423055746127 0.0003042560546754083\n",
      "6301 1.2465427681345318 0.00030407595541926996\n",
      "1 0.8441335130482912 0.00030391413923858634\n",
      "101 0.9350836968515068 0.00030373464611573535\n",
      "201 1.0421846130630001 0.0003035554706461405\n",
      "301 1.008769184758421 0.0003033766118939749\n",
      "401 0.9787611065548845 0.000303198068927267\n",
      "501 1.0469113910803571 0.00030301984081788013\n",
      "601 1.0306272330635693 0.00030284192664149214\n",
      "701 0.8391264111269265 0.00030266432547757535\n",
      "801 0.9852646276758605 0.00030248703640937665\n",
      "901 0.9640902730752714 0.00030231005852389745\n",
      "1001 1.1280414578068303 0.00030213339091187405\n",
      "1101 0.9939612101297826 0.0003019570326677579\n",
      "1201 1.0867023059399799 0.00030178098288969626\n",
      "1301 1.0411206257122103 0.00030160524067951265\n",
      "1401 1.0711584523378406 0.0003014298051426879\n",
      "1501 1.0796883448783774 0.0003012546753883405\n",
      "1601 1.027666941517964 0.00030107985052920836\n",
      "1701 1.082986782770604 0.00030090532968162913\n",
      "1801 1.1074479344606516 0.0003007311119655219\n",
      "1901 1.1305997944582487 0.0003005571965043686\n",
      "2001 1.345339710366943 0.0003003835824251953\n",
      "2101 1.001590578132891 0.00030021026885855383\n",
      "2201 1.0054450589232147 0.0003000372549385033\n",
      "2301 1.0041432639409322 0.00029986453980259265\n",
      "2401 1.0640304164699046 0.00029969212259184163\n",
      "2501 1.1201181028736755 0.0002995200024507235\n",
      "2601 0.8765224074013531 0.00029934817852714696\n",
      "2701 1.0152945614827331 0.0002991766499724386\n",
      "2801 1.2956223709957158 0.0002990054159413251\n",
      "2901 1.097986907014274 0.0002988344755919157\n",
      "3001 1.2291080165232415 0.00029866382808568526\n",
      "3101 1.1140619127836544 0.0002984934725874564\n",
      "3201 1.0722678579004423 0.0002983234082653825\n",
      "3301 1.03946516571159 0.0002981536342909311\n",
      "3401 0.9876702070032479 0.0002979841498388662\n",
      "3501 0.8540887358831242 0.00029781495408723205\n",
      "3601 0.9894529741141014 0.00029764604621733594\n",
      "3701 1.0710785342380404 0.000297477425413732\n",
      "3801 1.2710673977526312 0.00029730909086420423\n",
      "3901 1.0929949116252828 0.0002971410417597504\n",
      "4001 1.1222996068827342 0.0002969732772945655\n",
      "4101 1.164539644116303 0.00029680579666602566\n",
      "4201 1.033734397671651 0.000296638599074672\n",
      "4301 1.093015514779836 0.0002964716837241944\n",
      "4401 1.0481613585725427 0.0002963050498214161\n",
      "4501 1.0937337041832507 0.00029613869657627706\n",
      "4601 1.1815662420112858 0.000295972623201819\n",
      "4701 1.1428916620570817 0.0002958068289141693\n",
      "4801 1.1009264337189961 0.0002956413129325257\n",
      "4901 0.8777621657354757 0.00029547607447914055\n",
      "5001 1.0156730558082927 0.00029531111277930595\n",
      "5101 0.9942513670539483 0.00029514642706133804\n",
      "5201 1.1302866424011881 0.00029498201655656206\n",
      "5301 1.0087051462905947 0.0002948178804992971\n",
      "5401 1.0660143050336046 0.0002946540181268415\n",
      "5501 1.0107977577135898 0.00029449042867945755\n",
      "5601 1.0777363086963305 0.0002943271114003569\n",
      "5701 0.856828257907182 0.00029416406553568584\n",
      "5801 1.1287360956775956 0.0002940012903345107\n",
      "5901 1.366358119645156 0.00029383878504880313\n",
      "6001 1.0964845723065082 0.00029367654893342604\n",
      "6101 1.34646458978159 0.00029351458124611887\n",
      "6201 1.2197050878312439 0.0002933528812474836\n",
      "6301 1.0830936049751472 0.0002931914482009704\n",
      "1 1.2537849597129025 0.00029304477550482497\n",
      "101 0.9655292083625682 0.00029288385030021\n",
      "201 1.1372923650196753 0.0002927231899204302\n",
      "301 0.9451354363700375 0.0002925627936399378\n",
      "401 0.9661671929707154 0.00029240266073596516\n",
      "501 1.0162687979172915 0.0002922427904885108\n",
      "601 0.9551542007829994 0.0002920831821803257\n",
      "701 0.9841917234880384 0.0002919238350969\n",
      "801 1.061543255826109 0.00029176474852644945\n",
      "901 0.985908080736408 0.0002916059217599022\n",
      "1001 1.0337736615701942 0.0002914473540908853\n",
      "1101 0.9899544979416532 0.0002912890448157118\n",
      "1201 1.1052642236463726 0.00029113099323336726\n",
      "1301 0.9609193275682628 0.00029097319864549706\n",
      "1401 1.0143777604680508 0.0002908156603563932\n",
      "1501 1.0131031578639522 0.0002906583776729816\n",
      "1601 1.0399196342332289 0.00029050134990480915\n",
      "1701 1.3567781529854983 0.00029034457636403104\n",
      "1801 1.1145936762022757 0.0002901880563653981\n",
      "1901 1.0984270876506343 0.0002900317892262443\n",
      "2001 1.0226630433771788 0.00028987577426647405\n",
      "2101 1.1856945900362916 0.0002897200108085499\n",
      "2201 1.125245438015554 0.00028956449817748025\n",
      "2301 1.126162831991678 0.00028940923570080693\n",
      "2401 0.971063018507266 0.00028925422270859307\n",
      "2501 0.8773902256507427 0.00028909945853341086\n",
      "2601 0.8763325407635421 0.0002889449425103295\n",
      "2701 1.1415061227562546 0.0002887906739769035\n",
      "2801 1.052460735765635 0.0002886366522731603\n",
      "2901 1.5205009760629764 0.00028848287674158846\n",
      "3001 0.9414957111439435 0.0002883293467271265\n",
      "3101 0.9435001520323567 0.0002881760615771502\n",
      "3201 1.1403545759221743 0.0002880230206414618\n",
      "3301 1.053262686386006 0.00028787022327227786\n",
      "3401 1.0832857484929264 0.000287717668824218\n",
      "3501 1.1019753235159442 0.00028756535665429354\n",
      "3601 1.055358653771691 0.0002874132861218958\n",
      "3701 1.0278627741499804 0.00028726145658878504\n",
      "3801 1.0083879251906183 0.0002871098674190792\n",
      "3901 1.0555589499126654 0.0002869585179792425\n",
      "4001 1.0509156602493022 0.00028680740763807453\n",
      "4101 1.0385481148259714 0.0002866565357666993\n",
      "4201 1.0417402729653986 0.0002865059017385537\n",
      "4301 1.082753369351849 0.0002863555049293774\n",
      "4401 1.0459589868987678 0.0002862053447172013\n",
      "4501 1.0592919351911405 0.00028605542048233684\n",
      "4601 1.1034397517796606 0.0002859057316073656\n",
      "4701 1.1311876591207692 0.00028575627747712837\n",
      "4801 1.070465801298269 0.00028560705747871445\n",
      "4901 1.199700104945805 0.00028545807100145134\n",
      "5001 1.2379299406893551 0.00028530931743689397\n",
      "5101 1.1045849512156565 0.00028516079617881457\n",
      "5201 0.9958119990806154 0.000285012506623192\n",
      "5301 1.620139996672151 0.00028486444816820157\n",
      "5401 1.0613090786646353 0.0002847166202142048\n",
      "5501 1.175794189737644 0.0002845690221637393\n",
      "5601 1.1241089710965753 0.00028442165342150834\n",
      "5701 1.160597581154434 0.000284274513394371\n",
      "5801 1.0310369414401066 0.0002841276014913322\n",
      "5901 0.9960121748881647 0.0002839809171235324\n",
      "6001 0.9698299318188219 0.00028383445970423817\n",
      "6101 1.1415085992775857 0.0002836882286488319\n",
      "6201 1.0641657677479088 0.0002835422233748022\n",
      "6301 1.0828722650112468 0.00028339644330173413\n"
     ]
    }
   ],
   "source": [
    "#weight = torch.ones(len(TGT.vocab))\n",
    "#weight[pad_idx] = 0\n",
    "#criterion = nn.NLLLoss(size_average=False, weight=weight.cuda())\n",
    "criterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, label_smoothing=0.1)\n",
    "criterion.cuda()\n",
    "for epoch in range(15):\n",
    "    train_epoch(train_iter, model, criterion, model_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "35JC6i9QUgSB"
   },
   "outputs": [],
   "source": [
    "1 10.825187489390373 6.987712429686844e-07\n",
    "101 9.447168171405792 3.56373333914029e-05\n",
    "201 7.142856806516647 7.057589553983712e-05\n",
    "301 6.237934365868568 0.00010551445768827134\n",
    "401 5.762486848048866 0.00014045301983670557\n",
    "501 5.415792358107865 0.00017539158198513977\n",
    "601 5.081815680023283 0.000210330144133574\n",
    "701 4.788327748770826 0.00024526870628200823\n",
    "801 4.381739928154275 0.0002802072684304424\n",
    "901 4.55433791608084 0.00031514583057887664\n",
    "1001 4.911875109748507 0.0003500843927273108\n",
    "1101 4.0579032292589545 0.0003850229548757451\n",
    "1201 4.2276234351193125 0.0004199615170241793\n",
    "1301 3.932735869428143 0.00045490007917261356\n",
    "1401 3.8179439397063106 0.0004898386413210477\n",
    "1501 3.3608515430241823 0.000524777203469482\n",
    "1601 3.832796103321016 0.0005597157656179162\n",
    "1701 2.907085266895592 0.0005946543277663504\n",
    "1801 3.5280659823838505 0.0006295928899147847\n",
    "1901 2.895841649500653 0.0006645314520632189\n",
    "2001 3.273784235585481 0.000699470014211653\n",
    "2101 3.181488689899197 0.0007344085763600873\n",
    "2201 3.4151616653980454 0.0007693471385085215\n",
    "2301 3.4343731447588652 0.0008042857006569557\n",
    "2401 3.0505455391539726 0.0008392242628053899\n",
    "2501 2.8089329147478566 0.0008741628249538242\n",
    "2601 2.7827929875456903 0.0009091013871022583\n",
    "2701 2.4428516102489084 0.0009440399492506926\n",
    "2801 2.4015486147254705 0.0009789785113991267\n",
    "2901 2.3568112018401735 0.001013917073547561\n",
    "3001 2.6349758653668687 0.0010488556356959952\n",
    "3101 2.5981983028614195 0.0010837941978444295\n",
    "3201 2.666826274838968 0.0011187327599928637\n",
    "3301 3.0092043554177508 0.0011536713221412978\n",
    "3401 2.4580375660589198 0.0011886098842897321\n",
    "3501 2.586465588421561 0.0012235484464381662\n",
    "3601 2.5663993963389657 0.0012584870085866006\n",
    "3701 2.9430236657499336 0.0012934255707350347\n",
    "3801 2.464644919440616 0.001328364132883469\n",
    "3901 2.7124062888276512 0.0013633026950319032\n",
    "4001 2.646443709731102 0.0013971932312809247\n",
    "4101 2.7294750874862075 0.001380057517579748\n",
    "4201 2.1295202329056337 0.0013635372009002666\n",
    "4301 2.596563663915731 0.001347596306985731\n",
    "4401 2.1265982036820787 0.0013322017384983986\n",
    "4501 2.3880532500334084 0.0013173229858148\n",
    "4601 2.6129120760888327 0.0013029318725783852\n",
    "4701 2.2873719420749694 0.001289002331178292\n",
    "4801 2.4949760700110346 0.0012755102040816328\n",
    "4901 2.496607314562425 0.001262433067573089\n",
    "5001 2.1889712483389303 0.0012497500749750088\n",
    "5101 1.8677761815488338 0.0012374418168536253\n",
    "5201 2.2992054556962103 0.0012254901960784316\n",
    "5301 2.664361578106707 0.0012138783159049418\n",
    "5401 2.705850490485318 0.0012025903795063202\n",
    "5501 2.581445264921058 0.0011916115995949978\n",
    "5601 2.2480602325085783 0.0011809281169581616\n",
    "5701 1.9289666265249252 0.0011705269268863989\n",
    "5801 2.4863578918157145 0.0011603958126073107\n",
    "5901 2.632946971571073 0.0011505232849492607\n",
    "6001 2.496141305891797 0.0011408985275576757\n",
    "6101 2.6422974687084206 0.0011315113470699342\n",
    "6201 2.448802186456305 0.0011223521277270118"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "The Annotated \"Attention is All You Need\".ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
